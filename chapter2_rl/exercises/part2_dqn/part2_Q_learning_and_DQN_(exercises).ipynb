{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST7GZ0xkxW6j"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn.png\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzpGGRLH1tc8"
      },
      "source": [
        "Colab: [**exercises**](https://colab.research.google.com/drive/12YnrSDx0gbyhMKkcXoMvYZqu8oSgDEkM) | [**solutions**](https://colab.research.google.com/drive/1-b_RmzaGpnvKS_V6FFjylMYn4_JHVzXh)\n",
        "\n",
        "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), and ask any questions on the dedicated channels for this chapter of material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd3LpCav3UXu"
      },
      "source": [
        "# [2.2] - Q-Learning and DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HjUU8LBGTVG"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qXPOQMxGTVG"
      },
      "source": [
        "In this section, you'll implement Deep Q-Learning, often referred to as DQN for \"Deep Q-Network\". This was used in a landmark paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).\n",
        "\n",
        "At the time, the idea that convolutional neural networks could look at Atari game pixels and \"see\" gameplay-relevant features like a Space Invader was new and noteworthy. In 2022, we take for granted that convnets work, so we're going to focus on the RL aspect and not the vision aspect today.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfg2CtjDGTVH"
      },
      "source": [
        "## Content & Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHS98ZLGTVH"
      },
      "source": [
        "#### 1ï¸âƒ£ Q-Learning\n",
        "\n",
        "Now, we deal with situations where the environment is a black-box, and the agent must learn the rules of the world via interaction with it. This is different from everything else we've done so far, e.g. in the previous section we could calculate optimal policies by using the tensors $R$ and $T$, which we will now assume the agent doesn't have direct knowledge of.\n",
        "\n",
        "We call algorithms which have access to the transition probability distribution and reward function **model-based algorithms**. **Q-learning** is a **model-free algorithm**. From the original paper introducing Q-learning:\n",
        "\n",
        "*[Q-learning] provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.*\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> - Understand the basic Q-learning algorithm\n",
        "> - Implement SARSA and Q-Learning, and compare them on different envionments\n",
        "> - Understand the difference between model-based and model-free algorithms\n",
        "> - Learn more about exploration vs exploitation, and create an epsilon-greedy policy based on your Q-values\n",
        "\n",
        "#### 2ï¸âƒ£ DQN\n",
        "\n",
        "In this section, you'll implement Deep Q-Learning, often referred to as DQN for \"Deep Q-Network\". This was used in a landmark paper Playing Atari with [Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).\n",
        "\n",
        "You'll apply the technique of DQN to master the famous CartPole environment (below), and then (if you have time) move on to harder challenges like Acrobot and MountainCar.\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> - Understand the DQN algorithm\n",
        "> - Learn more about RL debugging, and build probe environments to debug your agents\n",
        "> - Create a replay buffer to store environment transitions\n",
        "> - Implement DQN using PyTorch Lightning, on the CartPole environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcgAnZZOyBYk"
      },
      "source": [
        "## Setup (don't read, just run!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yYsYe32yl9U"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install packages\n",
        "    %pip install wandb==0.13.10\n",
        "    %pip install einops\n",
        "    %pip install gym==0.23.1\n",
        "    %pip install pygame\n",
        "    %pip install jaxtyping\n",
        "\n",
        "    # Code to make sure output widgets display\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    import os, sys\n",
        "    if not os.path.exists(\"chapter2_rl\"):\n",
        "        !wget https://github.com/callummcdougall/ARENA_2.0/archive/refs/heads/main.zip\n",
        "        !unzip /content/main.zip 'ARENA_2.0-main/chapter2_rl/exercises/*'\n",
        "        sys.path.append(\"/content/ARENA_2.0-main/chapter2_rl/exercises\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_2.0-main/chapter2_rl\", \"chapter2_rl\")\n",
        "        os.rmdir(\"ARENA_2.0-main\")\n",
        "        os.chdir(\"chapter2_rl/exercises\")\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "# Things that need to be done manually\n",
        "\n",
        "# Nothing\n",
        "\n",
        "# END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRQ9j4ftyHXf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, List\n",
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "import gym.envs.registration\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm import tqdm, trange\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, List, Optional, Union, Tuple\n",
        "import torch as t\n",
        "from torch import nn, Tensor\n",
        "from gym.spaces import Discrete, Box\n",
        "from numpy.random import Generator\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from jaxtyping import Float, Int, Bool\n",
        "from IPython.display import clear_output\n",
        "\n",
        "Arr = np.ndarray\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = r\"chapter2_rl\"\n",
        "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
        "section_dir = exercises_dir / \"part2_dqn\"\n",
        "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
        "\n",
        "from part1_intro_to_rl.utils import make_env\n",
        "from part1_intro_to_rl.solutions import Environment, Toy, Norvig, find_optimal_policy\n",
        "import part2_dqn.utils as utils\n",
        "import part2_dqn.tests as tests\n",
        "from plotly_utils import line, cliffwalk_imshow, plot_cartpole_obs_and_dones\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAIN = __name__ == \"__main__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMHgKpawGTVI"
      },
      "source": [
        "# 1ï¸âƒ£ Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsm-QsurGTVI"
      },
      "source": [
        "> ##### Learning objectives\n",
        ">\n",
        "> - Understand the basic Q-learning algorithm\n",
        "> - Implement SARSA and Q-Learning, and compare them on different envionments\n",
        "> - Understand the difference between model-based and model-free algorithms\n",
        "> - Learn more about exploration vs exploitation, and create an epsilon-greedy policy based on your Q-values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rek2JToyGTVI"
      },
      "source": [
        "Now, we deal with situations where the environment is a black-box, and the agent must learn the rules of the world via interaction with it. This is different from everything else we've done so far, e.g. in the previous section we could calculate optimal policies by using the tensors $R$ and $T$, which we will now assume the agent doesn't have direct knowledge of.\n",
        "\n",
        "We call algorithms which have access to the transition probability distribution and reward function **model-based algorithms**. **Q-learning** is a **model-free algorithm**. From the original paper introducing Q-learning:\n",
        "\n",
        "*[Q-learning] provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.*\n",
        "\n",
        "The \"Q\" part of Q-learning refers to the function $Q$ which we encountered yesterday - the expected rewards for an action $a$ taken in a particular state $s$, based on some policy $\\pi$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8xiGsnnGTVJ"
      },
      "source": [
        "## Readings\n",
        "\n",
        "Don't worry about absorbing every detail, we will repeat a lot of the details here. Don't worry too much about the maths, we will also cover that here.\n",
        "\n",
        "- [Sutton and Barto](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
        "    - Chapter 6, Section 6.1, 6.3 (Especially Example 6.4)\n",
        "    - Note that Section 6.1 talks about temporal difference (TD) updates for the value function $V$. We will instead be using TD updates for the Q-value $Q$.\n",
        "    - Don't worry about the references to Monte Carlo in Chapter 5.\n",
        "\n",
        "### Optional Readings\n",
        "\n",
        "- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf) The original paper where Q-learning is first described.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGJkurZuGTVJ"
      },
      "source": [
        "Today and tomorrow, we'll be using OpenAI Gym, which provides a uniform interface to many different RL environments including Atari games. Gym was released in 2016 and details of the API have changed significantly over the years. We are using version 0.23.1, so ensure that any documentation you use refers to the same version.\n",
        "\n",
        "<details>\n",
        "<summary>What's the difference between observation and state?</summary>\n",
        "\n",
        "We use the word *observation* here as some environments are *partially observable*, the agent receives not an exact description of the state they are in, but merely an observation giving a partial description (for our gridworld, it could be a description of which cells directly adjacent to the agent are free to move into, rather than precisely which state they are in). This means that the agent would be unable to distinguish the cell north of the wall from the cell south of the wall. Returning the state as the observation is a special case, and we will often refer to one or the other as required.\n",
        "</details>\n",
        "\n",
        "Again, we'll be using NumPy for this section, and we'll start off with our gridworld environment from last week:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gridworld.png\" width=\"300\">\n",
        "\n",
        "but this time we'll use it within the `gym` framework.\n",
        "\n",
        "## Recap of `gym.Env`\n",
        "\n",
        "Let's have a speed recap of the key features the `gym.Env` class provides, and see how we can use it to wrap our gridworld environment from last week.\n",
        "\n",
        "### The `step` method\n",
        "\n",
        "The environment's `step` method takes the action selected by the agent and returns four values: `obs`, `reward`, `done`, and the `info` dictionary.\n",
        "\n",
        "`obs` and `reward` is the next observation and reward that the agent receives based on the action chosen.\n",
        "\n",
        "`done` indicates if the environment has entered a terminal state and ended. Here, both the goal-states (+1 and -1) are terminal. Early termination is equivalent to an infinite trajectory where the agent remains trapped for all future states, and always receives reward zero.\n",
        "\n",
        "`info` can contain anything extra that doesn't fit into the uniform interface - it's up to the environment what to put into it. A good use of this is for debugging information that the agent isn't \"supposed\" to see, like the dynamics of the environment. Agents that cheat and peek at `info` are helpful because we know that they should obtain the maximum possible rewards; if they aren't, then there's a bug. We will throw the entire underlying environment into `info`, from which an agent could cheat by peeking at the values for `T` and `R`.\n",
        "\n",
        "### The `render` method\n",
        "\n",
        "Render is only used for debugging or entertainment, and what it does is up to the environment. It might render a little popup window showing the Atari game, or it might give you a RGB image tensor, or just some ASCII text describing what's happening.\n",
        "\n",
        "### Observation and Action Types\n",
        "\n",
        "A `gym.Env` is a generic type: both the type of the observations and the type of the actions depends on the specifics of the environment.\n",
        "\n",
        "We're only dealing with the simplest case: a discrete set of actions which are the same in every state. In general, the actions could be continuous, or depend on the state.\n",
        "\n",
        "---\n",
        "\n",
        "Below, we define a class that allows us to use our old environment definition from last week, and wrap it in a `gym.Env` instance so we can learn from experience instead.\n",
        "\n",
        "Read the code below carefully and make sure you understand how the Gym environment API works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0F440psGTVJ"
      },
      "outputs": [],
      "source": [
        "ObsType = int\n",
        "ActType = int\n",
        "\n",
        "class DiscreteEnviroGym(gym.Env):\n",
        "    action_space: gym.spaces.Discrete\n",
        "    observation_space: gym.spaces.Discrete\n",
        "\n",
        "    def __init__(self, env: Environment):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.observation_space = gym.spaces.Discrete(env.num_states)\n",
        "        self.action_space = gym.spaces.Discrete(env.num_actions)\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        '''\n",
        "        Samples from the underlying dynamics of the environment\n",
        "        '''\n",
        "        (states, rewards, probs) = self.env.dynamics(self.pos, action)\n",
        "        idx = self.np_random.choice(len(states), p=probs)\n",
        "        (new_state, reward) = (states[idx], rewards[idx])\n",
        "        self.pos = new_state\n",
        "        done = self.pos in self.env.terminal\n",
        "        return (new_state, reward, done, {\"env\": self.env})\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        super().reset(seed=seed)\n",
        "        self.pos = self.env.start\n",
        "        return (self.pos, {\"env\": self.env}) if return_info else self.pos\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        assert mode == \"human\", f\"Mode {mode} not supported!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnsBRy1VGTVJ"
      },
      "source": [
        "### Registering an Environment\n",
        "\n",
        "User code normally won't use the constructor of an `Env` directly for two reasons:\n",
        "\n",
        "- Usually, we want to wrap our `Env` in one or more wrapper classes.\n",
        "- If we want to test our agent on a variety of environments, it's annoying to have to import all the `Env` classes directly.\n",
        "\n",
        "The `register` function stores information about our `Env` in a registry so that a later call to `gym.make` can look it up using the `id` string that is passed in.\n",
        "\n",
        "By convention, the `id` strings have a suffix with a version number. There can be multiple versions of the \"same\" environment with different parameters, and benchmarks should always report the version number for a fair comparison. For instance, `id=\"NorvigGrid-v0\"` below.\n",
        "\n",
        "### TimeLimit Wrapper\n",
        "\n",
        "As defined, our environment might take a very long time to terminate: A policy that actively avoids\n",
        "the terminal states and hides in the bottom-left corner would almost surely terminate through\n",
        "a long sequence of slippery moves, but this could take a long time.\n",
        "By setting `max_episode_steps` here, we cause our `env` to be wrapped in a `TimeLimit` wrapper class which terminates the episode after that number of steps.\n",
        "\n",
        "Note that the time limit is also an essential part of the problem definition: if it were larger or shorter, there would be more or less time to explore, which means that different algorithms (or at least different hyperparameters) would then have improved performance. We would obviously want to choose a rather\n",
        "conservative value such that any reasonably strong policy would be able to reach a terminal state in time.\n",
        "\n",
        "For our toy gridworld environment, we choose the (rather pessimistic) bound of 100 moves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1THGTm2GTVJ"
      },
      "outputs": [],
      "source": [
        "gym.envs.registration.register(\n",
        "    id=\"NorvigGrid-v0\",\n",
        "    entry_point=DiscreteEnviroGym,\n",
        "    max_episode_steps=100,\n",
        "    nondeterministic=True,\n",
        "    kwargs={\"env\": Norvig(penalty=-0.04)},\n",
        ")\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id=\"ToyGym-v0\",\n",
        "    entry_point=DiscreteEnviroGym,\n",
        "    max_episode_steps=2,\n",
        "    nondeterministic=False,\n",
        "    kwargs={\"env\": Toy()}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ8ea3cnGTVJ"
      },
      "source": [
        "Provided is the `RandomAgent` subclass which should pick an action at random, using the random number generator provided by gym. This is useful as a baseline to ensure the environment has no bugs. If your later agents are doing worse than random, you have a bug!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpHQItBsGTVJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    '''A class for storing one piece of experience during an episode run'''\n",
        "    obs: ObsType\n",
        "    act: ActType\n",
        "    reward: float\n",
        "    new_obs: ObsType\n",
        "    new_act: Optional[ActType] = None\n",
        "\n",
        "@dataclass\n",
        "class AgentConfig:\n",
        "    '''Hyperparameters for agents'''\n",
        "    epsilon: float = 0.1\n",
        "    lr: float = 0.05\n",
        "    optimism: float = 0\n",
        "\n",
        "defaultConfig = AgentConfig()\n",
        "\n",
        "class Agent:\n",
        "    '''Base class for agents interacting with an environment (you do not need to add any implementation here)'''\n",
        "    rng: np.random.Generator\n",
        "\n",
        "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
        "        self.env = env\n",
        "        self.reset(seed)\n",
        "        self.config = config\n",
        "        self.gamma = gamma\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.num_states = env.observation_space.n\n",
        "        self.name = type(self).__name__\n",
        "\n",
        "    def get_action(self, obs: ObsType) -> ActType:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def observe(self, exp: Experience) -> None:\n",
        "        '''\n",
        "        Agent observes experience, and updates model as appropriate.\n",
        "        Implementation depends on type of agent.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed: int) -> None:\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def run_episode(self, seed) -> List[int]:\n",
        "        '''\n",
        "        Simulates one episode of interaction, agent learns as appropriate\n",
        "        Inputs:\n",
        "            seed : Seed for the random number generator\n",
        "        Outputs:\n",
        "            The rewards obtained during the episode\n",
        "        '''\n",
        "        rewards = []\n",
        "        obs = self.env.reset(seed=seed)\n",
        "        self.reset(seed=seed)\n",
        "        done = False\n",
        "        while not done:\n",
        "            act = self.get_action(obs)\n",
        "            (new_obs, reward, done, info) = self.env.step(act)\n",
        "            exp = Experience(obs, act, reward, new_obs)\n",
        "            self.observe(exp)\n",
        "            rewards.append(reward)\n",
        "            obs = new_obs\n",
        "        return rewards\n",
        "\n",
        "    def train(self, n_runs=500):\n",
        "        '''\n",
        "        Run a batch of episodes, and return the total reward obtained per episode\n",
        "        Inputs:\n",
        "            n_runs : The number of episodes to simulate\n",
        "        Outputs:\n",
        "            The discounted sum of rewards obtained for each episode\n",
        "        '''\n",
        "        all_rewards = []\n",
        "        for seed in trange(n_runs):\n",
        "            rewards = self.run_episode(seed)\n",
        "            all_rewards.append(utils.sum_rewards(rewards, self.gamma))\n",
        "        return all_rewards\n",
        "\n",
        "class Random(Agent):\n",
        "    def get_action(self, obs: ObsType) -> ActType:\n",
        "        return self.rng.integers(0, self.num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JHrTHLAGTVK"
      },
      "source": [
        "## Cheater Agent\n",
        "\n",
        "Just like yesterday, you'll implement a cheating agent that peeks at the info and finds the optimal policy directly using your previous code. If your agent gets more than this in the long run, you have a bug!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59P6NxujGTVK"
      },
      "source": [
        "### Exercise - implement `Cheater`\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhf8fSEzGTVK"
      },
      "source": [
        "You should solve for the optimal policy once when `Cheater` is initalised, and then use that to define `get_action`.\n",
        "\n",
        "Check that your cheating agent outperforms the random agent. The cheating agent represents the best possible behavior, as it omnisciently always knows to play optimally.\n",
        "\n",
        "On the environment `ToyGym-v0`, (assuming $\\gamma = 0.99$) the cheating agent should always get reward $2 \\gamma = 1.98$,\n",
        "and the random agent should get a fluctuating reward, with average $\\frac{2 \\gamma + 1}{2} = 1.49$.\n",
        "\n",
        "Hint: Use `env.unwrapped.env` to extract the `Environment` wrapped inside `gym.Env`, to get access to the underlying dynamics.\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get 'AttributeError: 'DiscreteEnviroGym' object has no attribute 'num_states''.</summary>\n",
        "\n",
        "This is probably because you're passing the `DiscreteEnviroGym` object to your `find_optimal_policy` function. In the following line of code:\n",
        "\n",
        "```python\n",
        "env_toy = gym.make(\"ToyGym-v0\")\n",
        "```\n",
        "\n",
        "the object `env_toy` wraps around the `Toy` environment you used last week. As mentioned, you'll need to use `env.unwrapped.env` to access this environment, and its dynamics.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure what I should be doing here.</summary>\n",
        "\n",
        "You should be using your `find_optimal_policy` function fom yesterday. Recall that the first argument of this function was the `Environment` object; you can find this by using `env.unwrapped.env`.\n",
        "\n",
        "When getting an action in response to an observation, you can just extract the corresponding action from this optimal policy (i.e. by indexing into your policy array). Remember that your policy is a 1D array containing the best action for each observation.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKnHBl20GTVK"
      },
      "outputs": [],
      "source": [
        "class Cheater(Agent):\n",
        "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma=0.99, seed=0):\n",
        "        super().__init__(env, config, gamma, seed)\n",
        "        pass\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        pass\n",
        "\n",
        "env_toy = gym.make(\"ToyGym-v0\")\n",
        "agents_toy: List[Agent] = [Cheater(env_toy), Random(env_toy)]\n",
        "returns_list = []\n",
        "names_list = []\n",
        "for agent in agents_toy:\n",
        "    returns = agent.train(n_runs=100)\n",
        "    returns_list.append(utils.cummean(returns))\n",
        "    names_list.append(agent.name)\n",
        "\n",
        "line(returns_list, names=names_list, title=f\"Avg. reward on {env_toy.spec.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTwe3vAqGTVK"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class Cheater(Agent):\n",
        "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma=0.99, seed=0):\n",
        "        super().__init__(env, config, gamma, seed)\n",
        "        # SOLUTION\n",
        "        self.pi_opt = find_optimal_policy(self.env.unwrapped.env, self.gamma)\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        # SOLUTION\n",
        "        return self.pi_opt[obs]\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2dCUtfmGTVK"
      },
      "source": [
        "## SARSA: On-Policy TD Control\n",
        "\n",
        "Now we wish to train an agent on the same gridworld environment as before, but this time it doesn't have access to the underlying dynamics (`T` and `R`). The rough idea here is to try and estimate the Q-value function directly from samples received from the environment. Recall that the optimal Q-value function satisfies\n",
        "$$\n",
        "Q^*(s,a) = \\mathbb{E}_{\\pi^*} \\left[ \\sum_{i=t+1}^\\infty \\gamma^{i-t}r_i  \\mid s_t = s, a_t = a\\right]\n",
        "= \\mathbb{E}_{\\pi^*} \\left[r + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
        "= \\sum_{s'} T(s' \\mid s,a) \\left( R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a') \\right)\n",
        "$$\n",
        "where\n",
        "* $s'$ represents the next state after $s$,\n",
        "* $a'$ the next action after $a$\n",
        "* $r$ is the reward obtained from taking action $a$ in state $s$\n",
        "* the expectation $\\mathbb{E}_{\\pi^*}$ is with respect to both the optimal policy $\\pi^*$, as well as the stochasticity in the environment itself.\n",
        "\n",
        "So, for any particular episode $s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3,\\ldots$ we have that\n",
        "*on average* the value of $Q^*(s_t, a_t)$ should be equal to the *actual reward*\n",
        "$r_t$ recieved when choosing action $a_t$ in state $s_t$, plus $\\gamma$ times the\n",
        "Q-value of the next state $s_{t+1}$ and next action $a_{t+1}$.\n",
        "$$\n",
        "Q^*(s_t,a_t) =\n",
        "\\mathbb{E}_{\\pi^*} \\left[r + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
        "\\approx r_{t+1} + \\gamma  Q^*(s_{t+1}, a_{t+1})\n",
        "$$\n",
        "where $a_{t+1} = \\pi^*(s_{t+1}) = \\text{argmax}_a Q^*(s_{t+1}, a)$.\n",
        "\n",
        "\n",
        "Letting $Q$ denote our best current estimate of $Q^*$, the error $\\delta_t := r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)$  in this \"guess\" is called the **TD error**, and tells us in which direction we should bump our estimate of $Q^*$.\n",
        "Of course, this estimate might be wildly inaccurate (even for the same state-action pair!), due to the stochasticity of the environment, or poor estimates of $Q$. So, we update our estimate slightly in the direction of $\\delta_t$, much like stochastic gradient descent does. The update rule for Q-learning (with learning rate $\\eta > 0$) is\n",
        "$$\n",
        "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\eta \\left( r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t) \\right)\n",
        "$$\n",
        "This update depends on the information $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$, and so is called **SARSA** learning. Note that SARSA learns *on-policy*, in that it only learns from data that was actually generated by the current policy $\\pi$, derived from the current estimate of $Q$, $\\pi(s) = \\text{argmax}_a Q(s,a)$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/sarsa.png\" width=\"700\">\n",
        "\n",
        "## Q-Learning: Off-policy TD Control\n",
        "\n",
        "At the end of the day, what SARSA is essentially doing is estimating $Q^\\pi$ by using the rewards gathered by following policy $\\pi$. But we don't actually care about $Q^\\pi$, what we care about is $Q^*$. Q-Learning provides a slight modification to SARSA, by modifying the TD-error $\\delta_t$ to use the action that $\\pi$ *should* have taken in state $s_t$ (namely $\\text{argmax}_a Q(s_t,a)$) rather than the action $a_t = \\pi(s_t)$ that was actually taken.\n",
        "$$\n",
        "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\eta \\left( r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t,a_t) \\right)\n",
        "$$\n",
        "Note that each Q-learning update depends on the information $(s_t, a_t, r_{t+1}, s_{t+1})$.\n",
        "This means that Q-learning tries to estimate $Q^*$ directly, regardless of what policy $\\pi$ generated the episode, and so Q-Learning learns *off-policy*.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/qlearn.png\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nT8IqLnGTVL"
      },
      "source": [
        "## Explore vs. Exploit\n",
        "\n",
        "Lastly, methods to learn the Q-value often have trouble exploring. If a state-action pair $(s,a)$ with low Q-value $Q^*(s,a)$ just so happens to return a high reward by chance, the greedy policy with respect to $Q$ will often choose action $a$ in state $s$ instead of exploring potentially other good actions first. To remedy this, we use instead an $\\epsilon$-greedy policy with respect to the current Q-value estimates: With probability $\\epsilon$, a random action is chosen, and with probability $1-\\epsilon$ the greedy action $\\text{argmax}_a Q(s,a)$ is chosen. The exploration probability $\\epsilon$ is a hyperparameter that for now we will set to a constant $\\epsilon = 0.1$, but more sophisticated techniques include the use of a schedule to start exploring often early, and then decay exploration as times goes on.\n",
        "\n",
        "We also have the choice of how the estimate $Q(s,a)$ is initialized. By choosing \"optimistic\" values (initial values that are much higher than what we expect $Q^*(s,a)$ to actually be), this will encourage the greedy policy to hop between different actions in each state when they discover they weren't as valuable as first thought.\n",
        "\n",
        "We will implement an `EpsilonGreedy` agent that keeps track of the current Q-value estimates, and selects an action based on the epsilon greedy policy.\n",
        "\n",
        "Both `SARSA` and `QLearning` will inherit from `EpsilonGreedy`, and differ in how they update the Q-value estimates.\n",
        "\n",
        "- Keep track of an estimate of the Q-value of each state-action pair.\n",
        "- Epsilon greedy exploration: with probability `epsilon`, take a random action; otherwise take the action with the highest average observed reward (according to your current Q-value estimates).\n",
        "    - Remember that your `AgentConfig` object contains epsilon, as well as the optimism value and learning rate.\n",
        "- Optimistic initial values: initialize each arm's reward estimate with the `optimism` value.\n",
        "- Compare the performance of your Q-learning and SARSA agent against the random and cheating agents.\n",
        "- Try and tweak the hyperparameters `epsilon`, `lr` and `optimism` from their default values to see what effect this has. How fast can you get your agents to perform?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_zSTYwnGTVM"
      },
      "source": [
        "### Exercise - Implement Q-learning and SARSA\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 25-45 minutes on these exercises.\n",
        "\n",
        "They are difficult, so you should use the hints if you need.\n",
        "```\n",
        "\n",
        "#### Tips\n",
        "\n",
        "- Use `self.rng.random()` to generate random numbers in the range $[0,1)$, and `self.rng.integers(0, n)` for random integers in the range $0, 1, \\ldots, n-1$.\n",
        "- The random agent results in very long episodes, which slows evaluation. You can remove them from the experiment once you've convinced yourself that your agents are doing something intelligent and outperforming a random agent.\n",
        "- Leave $\\gamma =0.99$ for now.\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure what methods I should be rewriting in QLearning and SARSA.</summary>\n",
        "\n",
        "Your `EpsilonGreedy` agent already has a method for getting actions, which should use the `self.Q` object. This will be the same for `QLearning` and `SARSA`.\n",
        "\n",
        "The code you need to add is the `observe` method. Recall from the code earlier that `observe` takes an `Experience` object, which stores data `obs`, `act`, `reward`, `new_obs`, and `new_act`. In mathematical notation, these correspond to $s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$ and $a_{t+1}$.\n",
        "\n",
        "For `SARSA`, there's an added complication. We want SARSA to directly update the Q-value estimates after action $a_{t+1}$ is taken, as we need all of $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ to perform a SARSA update. This means you will need to override the `run_episode` function to adjust when the Q-values are updated.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm still confused about the <code>run_episode</code> function.</summary>\n",
        "\n",
        "The main loop of the original `run_episode` function looked like this:\n",
        "\n",
        "```python\n",
        "while not done:\n",
        "    act = self.get_action(obs)\n",
        "    (new_obs, reward, done, info) = self.env.step(act)\n",
        "    exp = Experience(obs, act, reward, new_obs)\n",
        "    self.observe(exp)\n",
        "    rewards.append(reward)\n",
        "    obs = new_obs\n",
        "```\n",
        "\n",
        "The problem here is that we don't have `new_act` in our `Experience` dataclass, because we only keep track of one action at a time. We can fix this by defining `new_act = self.get_action(new_obs)` **after** `new_obs` is defined. In this way, we can pass all of `(obs, act, reward, new_obs, new_act)` into our `Experience` dataclass.\n",
        "\n",
        "```python\n",
        "while not done:\n",
        "    (new_obs, reward, done, info) = self.env.step(act)\n",
        "    new_act = self.get_action(new_obs)\n",
        "    exp = Experience(obs, act, reward, new_obs, new_act)\n",
        "    self.observe(exp)\n",
        "    rewards.append(reward)\n",
        "    obs = new_obs\n",
        "    act = new_act\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>What output should I expect to see?</summary>\n",
        "\n",
        "SARSA should outperform Q-Learning (by a lot at first, then this gap will narrow). They should both be slightly worse than the cheater, and *much* better than the random agent.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHcog_byGTVM"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(Agent):\n",
        "    '''\n",
        "    A class for SARSA and Q-Learning to inherit from.\n",
        "    '''\n",
        "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, obs: ObsType) -> ActType:\n",
        "        '''\n",
        "        Selects an action using epsilon-greedy with respect to Q-value estimates\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "class QLearning(EpsilonGreedy):\n",
        "    def observe(self, exp: Experience) -> None:\n",
        "        pass\n",
        "\n",
        "class SARSA(EpsilonGreedy):\n",
        "    def observe(self, exp: Experience):\n",
        "        pass\n",
        "\n",
        "    def run_episode(self, seed) -> List[int]:\n",
        "        pass\n",
        "\n",
        "n_runs = 1000\n",
        "gamma = 0.99\n",
        "seed = 1\n",
        "env_norvig = gym.make(\"NorvigGrid-v0\")\n",
        "config_norvig = AgentConfig()\n",
        "args_norvig = (env_norvig, config_norvig, gamma, seed)\n",
        "agents_norvig: List[Agent] = [Cheater(*args_norvig), QLearning(*args_norvig), SARSA(*args_norvig), Random(*args_norvig)]\n",
        "returns_norvig = {}\n",
        "fig = go.Figure(layout=dict(\n",
        "    title_text=f\"Avg. reward on {env_norvig.spec.name}\",\n",
        "    template=\"simple_white\",\n",
        "    xaxis_range=[-30, n_runs+30]\n",
        "))\n",
        "for agent in agents_norvig:\n",
        "    returns = agent.train(n_runs)\n",
        "    fig.add_trace(go.Scatter(y=utils.cummean(returns), name=agent.name))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3LGimwPGTVM"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class EpsilonGreedy(Agent):\n",
        "    '''\n",
        "    A class for SARSA and Q-Learning to inherit from.\n",
        "    '''\n",
        "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
        "        # SOLUTION\n",
        "        super().__init__(env, config, gamma, seed)\n",
        "        self.Q = np.zeros((self.num_states, self.num_actions)) + self.config.optimism\n",
        "\n",
        "    def get_action(self, obs: ObsType) -> ActType:\n",
        "        '''\n",
        "        Selects an action using epsilon-greedy with respect to Q-value estimates\n",
        "        '''\n",
        "        # SOLUTION\n",
        "        if self.rng.random() < self.config.epsilon:\n",
        "            return self.rng.integers(0, self.num_actions)\n",
        "        else:\n",
        "            return self.Q[obs].argmax()\n",
        "\n",
        "class QLearning(EpsilonGreedy):\n",
        "    def observe(self, exp: Experience) -> None:\n",
        "        # SOLUTION\n",
        "        s, a, r_new, s_new = exp.obs, exp.act, exp.reward, exp.new_obs\n",
        "        self.Q[s,a] += self.config.lr * (r_new + self.gamma * np.max(self.Q[s_new]) - self.Q[s, a])\n",
        "\n",
        "class SARSA(EpsilonGreedy):\n",
        "    def observe(self, exp: Experience):\n",
        "        # SOLUTION\n",
        "        s, a, r_new, s_new, a_new = exp.obs, exp.act, exp.reward, exp.new_obs, exp.new_act\n",
        "        self.Q[s,a] += self.config.lr * (r_new + self.gamma * self.Q[s_new, a_new] - self.Q[s, a])\n",
        "\n",
        "    def run_episode(self, seed) -> List[int]:\n",
        "        # SOLUTION\n",
        "        rewards = []\n",
        "        obs = self.env.reset(seed=seed)\n",
        "        act = self.get_action(obs)\n",
        "        self.reset(seed=seed)\n",
        "        done = False\n",
        "        while not done:\n",
        "            (new_obs, reward, done, info) = self.env.step(act)\n",
        "            new_act = self.get_action(new_obs)\n",
        "            exp = Experience(obs, act, reward, new_obs, new_act)\n",
        "            self.observe(exp)\n",
        "            rewards.append(reward)\n",
        "            obs = new_obs\n",
        "            act = new_act\n",
        "        return rewards\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKg_Z6JwGTVM"
      },
      "source": [
        "Compare the performance of SARSA and Q-Learning on the gridworld environment v.s. the cheating agent and the random agent. Try to tune the hyperparameters to get the best performance you can.\n",
        "\n",
        "- Which seems to work better? SARSA or Q-Learning?\n",
        "- Does the optimism parameter seems to help?\n",
        "- What's the best choice of exploration parameter $\\epsilon$?\n",
        "- The feedback from the environment is very noisy. At the moment, the code provided plots the cumulative average reward per episode. You might want to try plotting a sliding average instead, or an exponential weighted moving average (see `part2_dqn/utils.py`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7vuQBO1GTVM"
      },
      "source": [
        "## Other Environments\n",
        "\n",
        "`gym` provides a large set of environments with which to test agents against. We can see all available environments by running `gym.envs.registry.all()`\n",
        "\n",
        "Have a look at [the gym library](https://www.gymlibrary.dev/environments/toy_text/) for descriptions of these environments. As written, our SARSA and Q-Learning agents will only work with environments that have both discrete observation and discrete action spaces.\n",
        "\n",
        "We'll modify the above code to use environment `gym.make(\"CliffWalking-v0\")` instead (see [this link](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)). We have the following graph from Sutton & Barto, Example 6.6, that displays the sum of reward obtained for each episode, as well as the policies obtained (SARSA takes the safer path, Q-Learning takes the optimal path). You may want to check out [this post](https://towardsdatascience.com/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cliff_pi.png\" width=\"400\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cliff.png\" width=\"400\">\n",
        "\n",
        "Do you get a similar result when you run the code below?\n",
        "\n",
        "Some notes:\n",
        "\n",
        "* Use $\\gamma = 1$ as described in Sutton & Barto, Example 6.6.\n",
        "* Try tweaking the learning rate and epsilon (start with $\\epsilon = 0.1$) to try and cause SARSA to take the cautious path, while Q-Learning takes the risky path.\n",
        "* We've included some helper functions to display the value of each state, as well as the policy an agent would take, given the Q-value function.\n",
        "* One of the bonus exercises we've suggested is to write your own version of `CliffWalking-v0` by writing a class similar to the `Norvig` class you have been working with. If you do this correctly, then you'll also be able to make a cheating agent.\n",
        "* We've defined a `cliffwalk_imshow` helper function for you, which visualises your agent's path (and reward at each square).\n",
        "\n",
        "<details>\n",
        "<summary>Question - why is it okay to use gamma=1 here?</summary>\n",
        "\n",
        "The penalty term `-1` makes sure that the agent continually penalised until it hits the terminal state. Unlike our `Norvig` environment, there is no wall to get stuck in perpetually, rather hitting the cliff will send you back to the start, so the agent must eventually reach the terminal state.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rEcM3vRGTVM"
      },
      "outputs": [],
      "source": [
        "gamma = 1\n",
        "seed = 0\n",
        "\n",
        "config_cliff = AgentConfig(epsilon=0.1, lr = 0.1, optimism=0)\n",
        "env = gym.make(\"CliffWalking-v0\")\n",
        "n_runs = 2500\n",
        "args_cliff = (env, config_cliff, gamma, seed)\n",
        "\n",
        "returns_list = []\n",
        "name_list = []\n",
        "agents: List[Union[QLearning, SARSA]] = [QLearning(*args_cliff), SARSA(*args_cliff)]\n",
        "\n",
        "for agent in agents:\n",
        "    returns = agent.train(n_runs)[1:]\n",
        "    returns_list.append(utils.cummean(returns))\n",
        "    name_list.append(agent.name)\n",
        "    V = agent.Q.max(axis=-1).reshape(4, 12)\n",
        "    pi = agent.Q.argmax(axis=-1).reshape(4, 12)\n",
        "    cliffwalk_imshow(V, pi, title=f\"CliffWalking: {agent.name} Agent\")\n",
        "\n",
        "line(\n",
        "    returns_list,\n",
        "    names=name_list,\n",
        "    template=\"simple_white\",\n",
        "    title=\"Q-Learning vs SARSA on CliffWalking-v0\",\n",
        "    labels={\"x\": \"Episode\", \"y\": \"Avg. reward\", \"variable\": \"Agent\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5k80ZxlGTVN"
      },
      "source": [
        "## Tabular Methods\n",
        "\n",
        "The methods used here are called tabular methods, because they create a lookup table from `(state, action)` to the Q value. This is pure memorization, and if our reward function was sampled from the space of all functions, this is usually the best you can do because there's no structure that you can exploit to do better.\n",
        "\n",
        "We can hope to do better on most \"natural\" reward functions that do have structure. For example in a game of poker, there is structure in both of the actions (betting Â£100 will have a similar reward to betting Â£99 or Â£101), and between states (having a pair of threes in your hand is similar to having a pair of twos or fours). We need to take advantage of this, otherwise there are just too many states and actions to have any hope of training an agent.\n",
        "\n",
        "One idea is to use domain knowledge to hand-code a function that \"buckets\" states or actions into a smaller number of equivalence classes and use those as the states and actions in a smaller version of the problem (see Sutton and Barto, Section 9.5). This was one component in the RL agent [Libratus: The Superhuman AI for No-Limit Poker](https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf). The details are beyond the scope of today's material, but I found them fascinating.\n",
        "\n",
        "If you don't have domain knowledge to leverage, or you care specifically about making your algorithm \"general\", you can follow the approach that we'll be using in Part 2ï¸âƒ£: make a neural network that takes in a state (technically, an observation) and outputs a value for each action. We then train the neural network using environmental interaction as training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus - build your own CliffWalking environment"
      ],
      "metadata": {
        "id": "AyJO9GgmHLYa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-nyz5rGTVN"
      },
      "source": [
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ ðŸŸ \n",
        "Importance: ðŸŸ âšªâšªâšªâšª\n",
        "\n",
        "You should spend up to 30-60 minutes on this exercise, if you choose to do it.\n",
        "```\n",
        "\n",
        "You should return to this exercise at the end if you want to. For now, you should progress to part 2ï¸âƒ£.\n",
        "\n",
        "You can modify the code used to define the `Norvig` class to define your own version of `CliffWalking-v0`.\n",
        "\n",
        "You can do this without guidance, or you can get some more guidance from the dropdowns below. **Hint 1** offers vague guidance, **Hint 2** offers more specific direction.\n",
        "\n",
        "Some notes for this task:\n",
        "\n",
        "* The random agent will take a *very* long time to accidentally stumble into the goal state, and will slow down learning. You should probably neglect it.\n",
        "* As soon as you hit the cliff, you should immediately move back to the start square, i.e. in pseudocode:\n",
        "    ```python\n",
        "    if new_state in cliff:\n",
        "        new_state = start_state\n",
        "        reward -= 100\n",
        "    ```\n",
        "    This means you'll never calculate Q from the cliff, so your Q-values will always be zero here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7ivEiDIGTVN"
      },
      "outputs": [],
      "source": [
        "class CliffWalking(Environment):\n",
        "\n",
        "    def __init__(self, penalty=-1):\n",
        "        pass\n",
        "\n",
        "    def dynamics(self, state : int, action : int) -> Tuple[Arr, Arr, Arr]:\n",
        "        pass\n",
        "\n",
        "        def state_index(state):\n",
        "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
        "            pos = state[0] + state[1] * self.width\n",
        "            assert 0 <= pos < self.num_states, print(state, pos)\n",
        "            return pos\n",
        "\n",
        "        pos = self.states[state]\n",
        "        move = self.actions[action]\n",
        "\n",
        "        if state in self.terminal:\n",
        "            return (np.array([state]), np.array([0]), np.array([1]))\n",
        "\n",
        "        # No slipping; each action is deterministic\n",
        "        out_probs = np.zeros(self.num_actions)\n",
        "        out_probs[action] = 1\n",
        "\n",
        "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
        "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
        "        new_states = [pos + x for x in self.actions]\n",
        "\n",
        "        for i, s_new in enumerate(new_states):\n",
        "\n",
        "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
        "                out_states[i] = state\n",
        "                continue\n",
        "\n",
        "            new_state = state_index(s_new)\n",
        "\n",
        "            # Check if would hit the cliff, if so then get -100 penalty and go back to start\n",
        "            if new_state in self.cliff:\n",
        "                out_states[i] = self.start\n",
        "                out_rewards[i] -= 100\n",
        "\n",
        "            else:\n",
        "                out_states[i] = new_state\n",
        "\n",
        "            for idx in range(len(self.terminal)):\n",
        "                if new_state == self.terminal[idx]:\n",
        "                    out_rewards[i] = self.goal_rewards[idx]\n",
        "\n",
        "        return (out_states, out_rewards, out_probs)\n",
        "\n",
        "    @staticmethod\n",
        "    def render(Q: Arr, name: str):\n",
        "        V = Q.max(axis=-1).reshape(4, 12)\n",
        "        pi = Q.argmax(axis=-1).reshape(4, 12)\n",
        "        cliffwalk_imshow(V, pi, title=f\"CliffWalking: {name} Agent\")\n",
        "\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id=\"CliffWalking-myversion\",\n",
        "    entry_point=DiscreteEnviroGym,\n",
        "    max_episode_steps=200,\n",
        "    nondeterministic=True,\n",
        "    kwargs={\"env\": CliffWalking(penalty=-1)},\n",
        ")\n",
        "gamma = 0.99\n",
        "seed = 0\n",
        "config_cliff = AgentConfig(epsilon=0.1, lr = 0.1, optimism=0)\n",
        "env = gym.make(\"CliffWalking-myversion\")\n",
        "n_runs = 500\n",
        "args_cliff = (env, config_cliff, gamma,seed)\n",
        "\n",
        "agents = [Cheater(*args_cliff), QLearning(*args_cliff), SARSA(*args_cliff), Random(*args_cliff)]\n",
        "returns_list = []\n",
        "name_list = []\n",
        "\n",
        "for agent in agents:\n",
        "    returns = agent.train(n_runs)[1:]\n",
        "    returns_list.append(utils.cummean(returns))\n",
        "    name_list.append(agent.name)\n",
        "\n",
        "line(\n",
        "    returns_list,\n",
        "    names=name_list,\n",
        "    template=\"simple_white\",\n",
        "    title=\"Q-Learning vs SARSA on CliffWalking-v0\",\n",
        "    labels={\"x\": \"Episode\", \"y\": \"Avg. reward\", \"variable\": \"Agent\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hadPIh4GTVN"
      },
      "source": [
        "\n",
        "<details>\n",
        "<summary>Hints (for both methods)</summary>\n",
        "\n",
        "The main way in which the `CliffWalking` environment differs from the `Norvig` gridworld is that the former has cliffs while the latter has walls. Cliffs and walls have different behaviour; you can see how the cliffs affect the agent by visiting the documentation page for `CliffWalking-v0`.\n",
        "\n",
        "#### `__init__`\n",
        "\n",
        "This mainly just involves changing the dimensions of the space, position of the start and terminal states, and parameters like `penalty`. Also, rather than walls, you'll need to define the position of the **cliffs** (which behave differently).\n",
        "\n",
        "#### `dynamics`\n",
        "\n",
        "You'll need to modify `dynamics` in the following two ways:\n",
        "\n",
        "* Remove the slippage probability (although it would be interesting to experiment with this and see what effect it has!)\n",
        "* Remove the \"when you hit a wall, you get trapped forever\" behaviour, and replace it with \"when you hit a cliff, you get a reward of -100 and go back to the start state\".\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class CliffWalking(Environment):\n",
        "\n",
        "    def __init__(self, penalty=-1):\n",
        "        # SOLUTION\n",
        "        self.height = 4\n",
        "        self.width = 12\n",
        "        self.penalty = penalty\n",
        "        num_states = self.height * self.width\n",
        "        num_actions = 4\n",
        "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
        "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])  # up, right, down, left\n",
        "        self.dim = (self.height, self.width)\n",
        "\n",
        "        # special states: tuples of state and reward\n",
        "        # all other states get penalty\n",
        "        start = 36\n",
        "        terminal = np.array([47], dtype=int)\n",
        "        self.cliff = np.arange(37, 47, dtype=int)\n",
        "        self.goal_rewards = np.array([1.0, -1.0])\n",
        "\n",
        "        super().__init__(num_states, num_actions, start=start, terminal=terminal)\n",
        "\n",
        "\n",
        "    def dynamics(self, state : int, action : int) -> Tuple[Arr, Arr, Arr]:\n",
        "        # SOLUTION\n",
        "        def state_index(state):\n",
        "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
        "            pos = state[0] + state[1] * self.width\n",
        "            assert 0 <= pos < self.num_states, print(state, pos)\n",
        "            return pos\n",
        "\n",
        "        pos = self.states[state]\n",
        "        move = self.actions[action]\n",
        "\n",
        "        if state in self.terminal:\n",
        "            return (np.array([state]), np.array([0]), np.array([1]))\n",
        "\n",
        "        # No slipping; each action is deterministic\n",
        "        out_probs = np.zeros(self.num_actions)\n",
        "        out_probs[action] = 1\n",
        "\n",
        "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
        "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
        "        new_states = [pos + x for x in self.actions]\n",
        "\n",
        "        for i, s_new in enumerate(new_states):\n",
        "\n",
        "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
        "                out_states[i] = state\n",
        "                continue\n",
        "\n",
        "            new_state = state_index(s_new)\n",
        "\n",
        "            # Check if would hit the cliff, if so then get -100 penalty and go back to start\n",
        "            if new_state in self.cliff:\n",
        "                out_states[i] = self.start\n",
        "                out_rewards[i] -= 100\n",
        "\n",
        "            else:\n",
        "                out_states[i] = new_state\n",
        "\n",
        "            for idx in range(len(self.terminal)):\n",
        "                if new_state == self.terminal[idx]:\n",
        "                    out_rewards[i] = self.goal_rewards[idx]\n",
        "\n",
        "        return (out_states, out_rewards, out_probs)\n",
        "\n",
        "    @staticmethod\n",
        "    def render(Q: Arr, name: str):\n",
        "        V = Q.max(axis=-1).reshape(4, 12)\n",
        "        pi = Q.argmax(axis=-1).reshape(4, 12)\n",
        "        cliffwalk_imshow(V, pi, title=f\"CliffWalking: {name} Agent\")\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH7qU37cGTVN"
      },
      "source": [
        "\n",
        "\n",
        "### Monte-Carlo Q-learning\n",
        "\n",
        "Implement Monte-Carlo Q-learning (Chapter 5 Sutton and Barto) and $\\text{TD}(\\lambda)$ with eligibility traces (Chapter 7 Sutton and Barto).\n",
        "\n",
        "### LR scheduler\n",
        "\n",
        "Try using a schedule for the exploration rate $\\epsilon$ (Large values early to encourage exploration, low values later once the agent has sufficient statistics to play optimally).\n",
        "\n",
        "Would Q-Learning or SARSA be better off with a scheduled exploration rate?\n",
        "\n",
        "The Sutton book mentions that if $\\epsilon$ is gradually reduced, both methods asymptotically converge to the optimal policy. Is this what you find?\n",
        "\n",
        "### Other environments\n",
        "\n",
        "Try other environments like [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) and [BlackJack](https://www.gymlibrary.dev/environments/toy_text/blackjack/). Note that BlackJack uses `Tuple(Discrete(32), Discrete(11), Discrete(2))` as it's observation space, so you will have to write some glue code to convert this back and forth between an observation space of `Discrete(32 * 11 * 2)` to work with our agents as written.\n",
        "\n",
        "### Double-Q learning\n",
        "\n",
        "Read Sutton and Barto Section 6.7 Maximisation Bias and Double Learning. Implement Double-Q learning, and compare it's performance against SARSA and Q-Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIExEmUTGTVN"
      },
      "source": [
        "# 2ï¸âƒ£ Deep Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k61oraCIGTVQ"
      },
      "source": [
        "> ##### Learning objectives\n",
        ">\n",
        "> - Understand the DQN algorithm\n",
        "> - Learn more about RL debugging, and build probe environments to debug your agents\n",
        "> - Create a replay buffer to store environment transitions\n",
        "> - Implement DQN using PyTorch Lightning, on the CartPole environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGxbMmETGTVQ"
      },
      "source": [
        "In this section, you'll implement Deep Q-Learning, often referred to as DQN for \"Deep Q-Network\". This was used in a landmark paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).\n",
        "\n",
        "At the time, the paper was very exicitng: The agent would play the game by only looking at the same screen pixel data that a human player would be looking at, rather than a description of where the enemies in the game world are. The idea that convolutional neural networks could look at Atari game pixels and \"see\" gameplay-relevant features like a Space Invader was new and noteworthy. In 2022, we take for granted that convnets work, so we're going to focus on the RL aspect solely, and not the vision component.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZbv5_5GTVQ"
      },
      "source": [
        "## Readings\n",
        "\n",
        "* [Deep Q Networks Explained](https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained)\n",
        "    * A high-level distillation as to how DQN works.\n",
        "* [Andy Jones - Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html)\n",
        "    * Useful tips for debugging your code when it's not working.\n",
        "    * The \"probe environments\" (a collection of simple environments of increasing complexity) section will be our first line of defense against bugs.\n",
        "\n",
        "### Interesting Resources (not required reading)\n",
        "\n",
        "- [An Outsider's Tour of Reinforcement Learning](http://www.argmin.net/2018/06/25/outsider-rl/) - comparison of RL techniques with the engineering discipline of control theory.\n",
        "- [Towards Characterizing Divergence in Deep Q-Learning](https://arxiv.org/pdf/1903.08894.pdf) - analysis of what causes learning to diverge\n",
        "- [Divergence in Deep Q-Learning: Tips and Tricks](https://amanhussain.com/post/divergence-deep-q-learning/) - includes some plots of average returns for comparison\n",
        "- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) - 2017 bootcamp with video and slides. Good if you like videos.\n",
        "- [DQN debugging using OpenAI gym Cartpole](https://adgefficiency.com/dqn-debugging/) - random dude's adventures in trying to get it to work.\n",
        "- [CleanRL DQN](https://github.com/vwxyzjn/cleanrl) - single file implementations of RL algorithms. Your starter code today is based on this; try not to spoiler yourself by looking at the solutions too early!\n",
        "- [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html) - 2018 article describing difficulties preventing industrial adoption of RL.\n",
        "- [Deep Reinforcement Learning Works - Now What?](https://tesslerc.github.io/posts/drl_works_now_what/) - 2020 response to the previous article highlighting recent progress.\n",
        "- [Seed RL](https://github.com/google-research/seed_rl) - example of distributed RL using Docker and GCP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ynYEBivGTVR"
      },
      "source": [
        "## Conceptual overview of DQN\n",
        "\n",
        "DQN is the natural extension of Q-Learning into the domain of deep learning. The main difference is that, instead of a table to store all the Q-values for each state-action pair, we train a neural network to learn this function for us. The usual implementation (which we'll use here) is for the Q-network to take the state as input, and output a vector of Q-values for each action, i.e. we're learning the function $s \\to (Q(s, a_1), ..., Q(s, a_n))$.\n",
        "\n",
        "Below is an algorithm showing the conceptual overview of DQN. We cycle through the following process:\n",
        "\n",
        "* Generate a batch of experiences using our current policy, by epsilon-greedy sampling (i.e. we mostly take the action with the highest Q-value, but occasionally take a random action to encourage exploration).\n",
        "* Use these values to calculate a TD error, and update our network.\n",
        "    * To increase stability, we also have a **target network** we use for the \"next step\" part of the TD error. This is a lagged copy of the Q-network (we don't update its weights via gradient descent).\n",
        "* Repeat this until convergence.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-alg-conceptual-2.png\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj9t3ouWGTVR"
      },
      "source": [
        "## Fast Feedback Loops\n",
        "\n",
        "We want to have faster feedback loops, and learning from Atari pixels doesn't achieve that. It might take 15 minutes per training run to get an agent to do well on Breakout, and that's if your implementation is relatively optimized. Even waiting 5 minutes to learn Pong from pixels is going to limit your ability to iterate, compared to using environments that are as simple as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmvbu4hPGTVR"
      },
      "source": [
        "## CartPole\n",
        "\n",
        "The classic environment \"CartPole-v1\" is simple to understand, yet hard enough for a RL agent to be interesting, by the end of the day your agent will be able to do this and more! (Click to watch!)\n",
        "\n",
        "\n",
        "[![CartPole](https://img.youtube.com/vi/46wjA6dqxOM/0.jpg)](https://www.youtube.com/watch?v=46wjA6dqxOM \"CartPole\")\n",
        "\n",
        "If you like, run `python play_cartpole.py` (locally, not on the remote machine)\n",
        "to try having a go at the task yourself! Use Left/Right to move the cart, R to reset,\n",
        "and Q to quit. By default, the cart will alternate Left/Right actions (there's no no-op action)\n",
        "if you're not pressing a button.\n",
        "\n",
        "\n",
        "\n",
        "The description of the task is [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). Note that unlike the previous environments, the observation here is now continuous. You can see the source for CartPole [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py); don't worry about the implementation but do read the documentation to understand the format of the actions and observations.\n",
        "\n",
        "The simple physics involved would be very easy for a model-based algorithm to fit, (this is a common assignment in control theory using [proportional-integral-derivative](https://en.wikipedia.org/wiki/PID_controller) (PID) controllers) but today we're doing it model-free: your agent has no idea that these observations represent positions or velocities, and it has no idea what the laws of physics are. The network has to learn in which direction to bump the cart in response to the current state of the world.\n",
        "\n",
        "Each environment can have different versions registered to it. By consulting [the Gym source](https://github.com/openai/gym/blob/master/gym/envs/__init__.py) you can see that CartPole-v0 and CartPole-v1 are the same environment, except that v1 has longer episodes. Again, a minor change like this can affect what algorithms score well; an agent might consistently survive for 200 steps in an unstable fashion that means it would fall over if ran for 500 steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFyKMMbGTVR"
      },
      "source": [
        "## Outline of the Exercises\n",
        "\n",
        "- Implement the Q-network that maps a state to an estimated value for each action.\n",
        "- Implement the policy which chooses actions based on the Q-network, plus epsilon greedy randomness\n",
        "to encourage exploration.\n",
        "- Implement a replay buffer to store experiences $e_t = (s_t, a_t, r_{t+1}, s_{t+1})$.\n",
        "- Piece everything together into a training loop and train your agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zzQ4dLvGTVR"
      },
      "source": [
        "## The Q-Network\n",
        "\n",
        "The Q-Network takes in an observation and outputs a number for each available action predicting how good it is, mimicking he behaviour of our Q-value table from yesterday.\n",
        "For best results, the architecture of the Q-network can be customized to each particular problem. For example, [the architecture of OpenAI Five](https://cdn.openai.com/research-covers/openai-five/network-architecture.pdf) used to play DOTA 2 is pretty complex and involves LSTMs.\n",
        "\n",
        "For learning from pixels, a simple convolutional network and some fully connected layers does quite well. Where we have already processed features here, it's even easier: an MLP of this size should be plenty large for any environment today. Your code should support running the network on either GPU or CPU, but for CartPole it was actually faster to use CPU on my hardware.\n",
        "\n",
        "Implement the Q-network using a standard MLP, constructed of alternating Linear and ReLU layers.\n",
        "The size of the input will match the dimensionality of the observation space, and the size of the output will match the number of actions to choose from (associating a reward to each.)\n",
        "The dimensions of the hidden_sizes are provided.\n",
        "\n",
        "Here is a diagram of what our particular Q-Network will look like for CartPole:\n",
        "\n",
        "<figure style=\"max-width:250px\"><embed type=\"image/svg+xml\" src=\"https://mermaid.ink/svg/pako:eNplkEtrwzAMgP-K0WkDB5ash5LDTtmhUDa616UeRY3VxSx-4AdjlP73Oc3WpkwHIaTvA0l7aK0kqOHDo-vYSyMMyxHSdmw0RI6tigeKX9Z_jsMhFmYtYGFciuzKbsMmdOiIXwt4Z0Vxx5bKEPoyM2M1YVhZ3Zy4J1q-lushT73q7GWYs_nsQqj-CbdnYT7jzCS9wTYqa8JJXL1hn6nHFI87T5Dj1uNlZCRw0OQ1Kpmfsh_aAmJHmgTUuZS0w9RHAcIcMpqcxEj3UkXrod5hH4gDpmifv00LdfSJ_qBGYX6p_qUOP7mqd2g\" /></figure>\n",
        "\n",
        "<details>\n",
        "<summary>Why do we not include a ReLU at the end?</summary>\n",
        "\n",
        "If you end with a ReLU, then your network can only predict 0 or positive Q-values. This will cause problems as soon as you encounter an environment with negative rewards, or you try to do some scaling of the rewards.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>CartPole-v1 gives +1 reward on every timestep. Why would the network not just learn the constant +1 function regardless of observation?</summary>\n",
        "\n",
        "The network is learning Q-values (the sum of all future expected discounted rewards from this state/action pair), not rewards. Correspondingly, once the agent has learned a good policy, the Q-value associated with state action pair (pole is slightly left of vertical, move cart left) should be large, as we would expect a long episode (and correspondingly lots of reward) by taking actions to help to balance the pole. Pairs like (cart near right boundary, move cart right) cause the episode to terminate, and as such the network will learn low Q-values.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVoQRF0lGTVR"
      },
      "source": [
        "### Exercise - implement `QNetwork`\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BRMkBa2GTVR"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    '''For consistency with your tests, please wrap your modules in a `nn.Sequential` called `layers`.'''\n",
        "    layers: nn.Sequential\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_observation: int,\n",
        "        num_actions: int,\n",
        "        hidden_sizes: List[int] = [120, 84]\n",
        "    ):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        pass\n",
        "\n",
        "net = QNetwork(dim_observation=4, num_actions=2)\n",
        "n_params = sum((p.nelement() for p in net.parameters()))\n",
        "assert isinstance(getattr(net, \"layers\", None), nn.Sequential)\n",
        "print(net)\n",
        "print(f\"Total number of parameters: {n_params}\")\n",
        "print(\"You should manually verify network is Linear-ReLU-Linear-ReLU-Linear\")\n",
        "assert n_params == 10934"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faNHwgD5GTVS"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class QNetwork(nn.Module):\n",
        "    '''For consistency with your tests, please wrap your modules in a `nn.Sequential` called `layers`.'''\n",
        "    layers: nn.Sequential\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_observation: int,\n",
        "        num_actions: int,\n",
        "        hidden_sizes: List[int] = [120, 84]\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # SOLUTION\n",
        "        in_features_list = [dim_observation] + hidden_sizes\n",
        "        out_features_list = hidden_sizes + [num_actions]\n",
        "        layers = []\n",
        "        for i, (in_features, out_features) in enumerate(zip(in_features_list, out_features_list)):\n",
        "            layers.append(nn.Linear(in_features, out_features))\n",
        "            if i < len(in_features_list) - 1:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        # SOLUTION\n",
        "        return self.layers(x)\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_sAV6mZGTVS"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The goal of DQN is to reduce the reinforcement learning problem to a supervised learning problem.\n",
        "In supervised learning, training examples should be drawn **i.i.d**. from some distribution, and we hope to generalize to future examples from that distribution.\n",
        "\n",
        "In RL, the distribution of experiences $e_t = (s_t, a_t, r_{t+1}, s_{t+1})$ to train from depend on the policy $\\pi$ followed, which depends on the current state of the Q-value network, so DQN is always chasing a moving target. This is why the training loss curve isn't going to have a nice steady decrease like in supervised learning. We will extend experiences to $e_t = (o_t, a_t, r_{t+1}, o_{t+1}, d_{t+1})$. Here, $d_{t+1}$ is a boolean indicating that $o_{t+1}$ is a terminal observation, and that no further interaction happened beyond $s_{t+1}$ in the episode from which it was generated.\n",
        "\n",
        "### Correlated States\n",
        "\n",
        "Due to DQN using a neural network to learn the Q-values, the value of many state-action pairs are aggregated together (unlike tabular Q-learning which learns independently the value of each state-action pair). For example, consider a game of chess. The board will have some encoding as a vector, but visually similar board states might have wildly different consequences for the best move. Another problem is that states within an episode are highly correlated and not i.i.d. at all. A few bad moves from the start of the game might doom the rest of the game regardless how well the agent tries to recover, whereas a few bad moves near the end of the game might not matter if the agent has a very strong lead, or is so far behind the game is already lost. Training mostly on an episode where the agent opened the game poorly might disincentive good moves to recover, as these too will have poor Q-value estimates.\n",
        "\n",
        "### Uniform Sampling\n",
        "\n",
        "To recover from this problem and make the environment look \"more i.i.d\", a simple strategy that works decently well is to pick a buffer size, store experiences and uniformly sample out of that buffer. Intuitively, if we want the policy to play well in all sorts of states, the sampled batch should be a representative sample of all the diverse scenarios that can happen in the environment.\n",
        "\n",
        "For complex environments, this implies a very large batch size (or doing something better than uniform sampling). [OpenAI Five](https://cdn.openai.com/dota-2.pdf) used batch sizes of over 2 million experiences for Dota 2.\n",
        "\n",
        "The capacity of the replay buffer is yet another hyperparameter; if it's too small then it's just going to be full of recent and correlated examples. But if it's too large, we pay an increasing cost in memory usage and the information may be too old to be relevant.\n",
        "\n",
        "Implement `ReplayBuffer`. It only needs to handle a discrete action space, and you can assume observations are some shape of dtype `np.float32`, and actions are of dtype `np.int64`. The replay buffer will store experiences $e_t = (o_t, a_t, r_{t+1}, o_{t+1}, d_{t+1})$ in a circular queue. If the buffer is already full, the oldest experience is overwritten.\n",
        "\n",
        "You should also include objects `self.observations`, `self.actions`, etc in your `ReplayBuffer` class. This is just so that you can plot them against your shuffled replay buffer, and verify that the outputs look reasonable (see the next section).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_4PI9acGTVS"
      },
      "source": [
        "### Exercise - implement `ReplayBuffer`\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 20-30 minutes on this exercise.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b3FjURuGTVS"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ReplayBufferSamples:\n",
        "    '''\n",
        "    Samples from the replay buffer, converted to PyTorch for use in neural network training.\n",
        "    '''\n",
        "    observations: Float[Tensor, \"sampleSize *obsShape\"]\n",
        "    actions: Int[Tensor, \"sampleSize\"]\n",
        "    rewards: Float[Tensor, \"sampleSize\"]\n",
        "    dones: Bool[Tensor, \"sampleSize\"]\n",
        "    next_observations: Float[Tensor, \"sampleSize *obsShape\"]\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    '''\n",
        "    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.\n",
        "    '''\n",
        "    rng: Generator\n",
        "    observations: t.Tensor\n",
        "    actions: t.Tensor\n",
        "    rewards: t.Tensor\n",
        "    dones: t.Tensor\n",
        "    next_observations: t.Tensor\n",
        "\n",
        "    def __init__(self, buffer_size: int, num_environments: int, seed: int):\n",
        "        assert num_environments == 1, \"This buffer only supports SyncVectorEnv with 1 environment inside.\"\n",
        "        self.num_environments = num_environments\n",
        "        pass\n",
        "\n",
        "    def add(\n",
        "        self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray, dones: np.ndarray, next_obs: np.ndarray\n",
        "    ) -> None:\n",
        "        '''\n",
        "        obs: shape (num_environments, *observation_shape)\n",
        "            Observation before the action\n",
        "        actions: shape (num_environments,)\n",
        "            Action chosen by the agent\n",
        "        rewards: shape (num_environments,)\n",
        "            Reward after the action\n",
        "        dones: shape (num_environments,)\n",
        "            If True, the episode ended and was reset automatically\n",
        "        next_obs: shape (num_environments, *observation_shape)\n",
        "            Observation after the action\n",
        "            If done is True, this should be the terminal observation, NOT the first observation of the next episode.\n",
        "        '''\n",
        "        assert obs.shape[0] == self.num_environments\n",
        "        assert actions.shape == (self.num_environments,)\n",
        "        assert rewards.shape == (self.num_environments,)\n",
        "        assert dones.shape == (self.num_environments,)\n",
        "        assert next_obs.shape[0] == self.num_environments\n",
        "\n",
        "        pass\n",
        "\n",
        "    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:\n",
        "        '''\n",
        "        Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device.\n",
        "        Sampling is with replacement, and sample_size may be larger than the buffer size.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "tests.test_replay_buffer_single(ReplayBuffer)\n",
        "tests.test_replay_buffer_deterministic(ReplayBuffer)\n",
        "tests.test_replay_buffer_wraparound(ReplayBuffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmkW-r6iGTVS"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class ReplayBuffer:\n",
        "    '''\n",
        "    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.\n",
        "    '''\n",
        "    rng: Generator\n",
        "    observations: t.Tensor\n",
        "    actions: t.Tensor\n",
        "    rewards: t.Tensor\n",
        "    dones: t.Tensor\n",
        "    next_observations: t.Tensor\n",
        "\n",
        "    def __init__(self, buffer_size: int, num_environments: int, seed: int):\n",
        "        assert num_environments == 1, \"This buffer only supports SyncVectorEnv with 1 environment inside.\"\n",
        "        self.num_environments = num_environments\n",
        "        # SOLUTION\n",
        "        self.buffer_size = buffer_size\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.buffer = [None for _ in range(5)]\n",
        "\n",
        "    def add(\n",
        "        self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray, dones: np.ndarray, next_obs: np.ndarray\n",
        "    ) -> None:\n",
        "        '''\n",
        "        obs: shape (num_environments, *observation_shape)\n",
        "            Observation before the action\n",
        "        actions: shape (num_environments,)\n",
        "            Action chosen by the agent\n",
        "        rewards: shape (num_environments,)\n",
        "            Reward after the action\n",
        "        dones: shape (num_environments,)\n",
        "            If True, the episode ended and was reset automatically\n",
        "        next_obs: shape (num_environments, *observation_shape)\n",
        "            Observation after the action\n",
        "            If done is True, this should be the terminal observation, NOT the first observation of the next episode.\n",
        "        '''\n",
        "        assert obs.shape[0] == self.num_environments\n",
        "        assert actions.shape == (self.num_environments,)\n",
        "        assert rewards.shape == (self.num_environments,)\n",
        "        assert dones.shape == (self.num_environments,)\n",
        "        assert next_obs.shape[0] == self.num_environments\n",
        "\n",
        "        # SOLUTION\n",
        "        for i, (arr, arr_list) in enumerate(zip([obs, actions, rewards, dones, next_obs], self.buffer)):\n",
        "            if arr_list is None:\n",
        "                self.buffer[i] = arr\n",
        "            else:\n",
        "                self.buffer[i] = np.concatenate((arr, arr_list))\n",
        "            if self.buffer[i].shape[0] > self.buffer_size:\n",
        "                self.buffer[i] = self.buffer[i][:self.buffer_size]\n",
        "\n",
        "        self.observations, self.actions, self.rewards, self.dones, self.next_observations = [t.as_tensor(arr) for arr in self.buffer]\n",
        "\n",
        "    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:\n",
        "        '''\n",
        "        Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device.\n",
        "        Sampling is with replacement, and sample_size may be larger than the buffer size.\n",
        "        '''\n",
        "        # SOLUTION\n",
        "        indices = self.rng.integers(0, self.buffer[0].shape[0], sample_size)\n",
        "        samples = [t.as_tensor(arr_list[indices], device=device) for arr_list in self.buffer]\n",
        "        return ReplayBufferSamples(*samples)\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcj32lyMGTVT"
      },
      "source": [
        "## Environment Resets\n",
        "\n",
        "There's a subtlety to the Gym API around what happens when the agent fails and the episode is terminated. Our environment is set up to automatically reset at the end of an episode, but when this happens the `next_obs` returned from `step` is actually the initial observation of the new episode.\n",
        "\n",
        "What we want to store in the replay buffer is the final observation of the old episode. The code to do this is shown below.\n",
        "\n",
        "- Run the code and inspect the replay buffer contents. Referring back to the [CartPole source](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), do the numbers make sense?\n",
        "- Look at the sample, and see if it looks properly shuffled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMr8wt-qGTVT"
      },
      "outputs": [],
      "source": [
        "rb = ReplayBuffer(buffer_size=256, num_environments=1, seed=0)\n",
        "envs = gym.vector.SyncVectorEnv([make_env(\"CartPole-v1\", 0, 0, False, \"test\")])\n",
        "obs = envs.reset()\n",
        "for i in range(256):\n",
        "    actions = np.array([0])\n",
        "    (next_obs, rewards, dones, infos) = envs.step(actions)\n",
        "    rb.add(obs, actions, rewards, dones, next_obs)\n",
        "    obs = next_obs\n",
        "\n",
        "plot_cartpole_obs_and_dones(rb.observations.flip(0), rb.dones.flip(0))\n",
        "\n",
        "sample = rb.sample(256, t.device(\"cpu\"))\n",
        "plot_cartpole_obs_and_dones(sample.observations.flip(0), sample.dones.flip(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGd-s4DPGTVT"
      },
      "source": [
        "<details>\n",
        "<summary>You should be getting graphs which look like this:</summary>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/shuffled_and_un.png\" width=\"800\">\n",
        "\n",
        "Explanations - each of the dotted lines (the values $t^*$ where $d_{t^*}=1$) corresponds to a state $s_{t^*}$ where the pole's angle goes over the [bounds](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) of `+=0.2095` (note that it doesn't stay upright nearly long enough to hit the positional bounds). If you zoom in on one of these points, then you'll see that we never actually record observations when the pole is out of bounds. At $s_{t^*-1}$ we are still within bounds, and once we go over bounds the next observation is taken from the reset environment.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfGRGz3_GTVT"
      },
      "source": [
        "## Exploration\n",
        "\n",
        "DQN makes no attempt to explore intelligently. The exploration strategy is the same as\n",
        "for Q-Learning: agents take a random action with probability epsilon, but now we gradually\n",
        "decrease epsilon. The Q-network is also randomly initialized (rather than initialized with zeros),\n",
        "so its predictions of what is the best action to take are also pretty random to start.\n",
        "\n",
        "Some games like [Montezuma's Revenge](https://paperswithcode.com/task/montezumas-revenge) have sparse rewards that require more advanced exploration methods to obtain. The player is required to collect specific keys to unlock specific doors, but unlike humans, DQN has no prior knowledge about what a key or a door is, and it turns out that bumbling around randomly has too low of a probability of correctly matching a key to its door. Even if the agent does manage to do this, the long separation between finding the key and going to the door makes it hard to learn that picking the key up was important.\n",
        "\n",
        "As a result, DQN scored an embarrassing 0% of average human performance on this game.\n",
        "\n",
        "### Reward Shaping\n",
        "\n",
        "One solution to sparse rewards is to use human knowledge to define auxillary reward functions that are more dense and made the problem easier (in exchange for leaking in side knowledge and making\n",
        "the algorithm more specific to the problem at hand). What could possibly go wrong?\n",
        "\n",
        "The canonical example is for a game called [CoastRunners](https://openai.com/blog/faulty-reward-functions/), where the goal was given to maximize the\n",
        "score (hoping that the agent would learn to race around the map). Instead, it found it could\n",
        "gain more score by driving in a loop picking up power-ups just as they respawn, crashing and\n",
        "setting the boat alight in the process.\n",
        "\n",
        "### Reward Hacking\n",
        "\n",
        "For Montezuma's Revenge, the reward was shaped by giving a small reward for\n",
        "picking up the key.\n",
        "One time this was tried, the reward was given slightly too early and the agent learned it could go close to the key without quite picking it up, obtain the auxillary reward, and then back up and repeat.\n",
        "\n",
        "[![Montezuma Reward Hacking](https://img.youtube.com/vi/_sFp1ffKIc8/0.jpg)](https://www.youtube.com/watch?v=_sFp1ffKIc8 \"Montezuma Reward Hacking\")\n",
        "\n",
        "A collected list of examples of Reward Hacking can be found [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml).\n",
        "\n",
        "\n",
        "### Advanced Exploration\n",
        "\n",
        "It would be better if the agent didn't require these auxillary rewards to be hardcoded by humans,\n",
        "but instead reply on other signals from the environment that a state might be worth exploring. One idea is that a state which is \"surprising\" or \"novel\" (according to the agent's current belief\n",
        "of how the environment works) in some sense might be valuable. Designing an agent to be\n",
        "innately curious presents a potential solution to exploration, as the agent will focus exploration\n",
        "in areas it is unfamiliar with. In 2018, OpenAI released [Random Network Distillation](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/) which made progress in formalizing this notion, by measuring the agent's ability to predict the output of a neural network\n",
        "on visited states. States that are hard to predict are poorly explored, and thus highly rewarded.\n",
        "In 2019, an excellent paper [First return, then explore](https://arxiv.org/pdf/2004.12919v6.pdf) found an even better approach. Such reward shaping can also be gamed, leading to the\n",
        "noisy TV problem, where agents that seek novelty become entranced by a source of randomness in the\n",
        "environment (like a analog TV out of tune displaying white noise), and ignore everything else\n",
        "in the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBcBARRGTVT"
      },
      "source": [
        "### Exercise - implement linear scheduler\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ âšªâšªâšª\n",
        "\n",
        "You should spend up to 5-10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "For now, implement the basic linearly decreasing exploration schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF3lkMpCGTVT"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(\n",
        "    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int\n",
        ") -> float:\n",
        "    '''Return the appropriate epsilon for the current step.\n",
        "\n",
        "    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).\n",
        "\n",
        "    It should stay at end_e for the rest of the episode.\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "epsilons = [\n",
        "    linear_schedule(step, start_e=1.0, end_e=0.05, exploration_fraction=0.5, total_timesteps=500)\n",
        "    for step in range(500)\n",
        "]\n",
        "line(epsilons, labels={\"x\": \"steps\", \"y\": \"epsilon\"}, title=\"Probability of random action\")\n",
        "\n",
        "tests.test_linear_schedule(linear_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMASr-1nGTVT"
      },
      "source": [
        "<details>\n",
        "<summary>Plot of the Intended Schedule</summary>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/newplot.png\" width=\"560\">\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def linear_schedule(\n",
        "    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int\n",
        ") -> float:\n",
        "    '''Return the appropriate epsilon for the current step.\n",
        "\n",
        "    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).\n",
        "\n",
        "    It should stay at end_e for the rest of the episode.\n",
        "    '''\n",
        "    # SOLUTION\n",
        "    return start_e + (end_e - start_e) * min(current_step / (exploration_fraction * total_timesteps), 1)\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN8vcvizGTVU"
      },
      "source": [
        "## Epsilon Greedy Policy\n",
        "\n",
        "In DQN, the policy is implicitly defined by the Q-network: we take the action with the maximum predicted reward. This gives a bias towards optimism. By estimating the maximum of a set of values $v_1, \\ldots, v_n$ using the maximum of some noisy estimates $\\hat{v}_1, \\ldots, \\hat{v}_n$ with $\\hat{v}_i \\approx v$, we get unlucky and get very large positive noise on some samples, which the maximum then chooses. Hence, the agent will choose actions that the Q-network is overly optimistic about.\n",
        "\n",
        "See Sutton and Barto, Section 6.7 if you'd like a more detailed explanation, or the original [Double Q-Learning](https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf) paper which notes this maximisation bias, and introduces a method to correct for it using two separate Q-value estimators, each used to update the other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ixm0_HGTVU"
      },
      "source": [
        "### Exercise - implement the epsilon greedy policy\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "- Don't forget to convert the result back to a `np.ndarray`.\n",
        "- Use `rng.random()` to generate random numbers in the range $[0,1)$, and `rng.integers(0, n, size)` for an array of shape `size` random integers in the range $0, 1, \\ldots, n-1$.\n",
        "- Use `envs.single_action_space.n` to retrieve the number of possible actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAVv53OnGTVU"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(\n",
        "    envs: gym.vector.SyncVectorEnv, q_network: QNetwork, rng: Generator, obs: t.Tensor, epsilon: float\n",
        ") -> np.ndarray:\n",
        "    '''With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.\n",
        "    Inputs:\n",
        "        envs : gym.vector.SyncVectorEnv, the family of environments to run against\n",
        "        q_network : QNetwork, the network used to approximate the Q-value function\n",
        "        obs : The current observation\n",
        "        epsilon : exploration percentage\n",
        "    Outputs:\n",
        "        actions: (n_environments, ) the sampled action for each environment.\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "tests.test_epsilon_greedy_policy(epsilon_greedy_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPFyHFRYGTVU"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def epsilon_greedy_policy(\n",
        "    envs: gym.vector.SyncVectorEnv, q_network: QNetwork, rng: Generator, obs: t.Tensor, epsilon: float\n",
        ") -> np.ndarray:\n",
        "    '''With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.\n",
        "    Inputs:\n",
        "        envs : gym.vector.SyncVectorEnv, the family of environments to run against\n",
        "        q_network : QNetwork, the network used to approximate the Q-value function\n",
        "        obs : The current observation\n",
        "        epsilon : exploration percentage\n",
        "    Outputs:\n",
        "        actions: (n_environments, ) the sampled action for each environment.\n",
        "    '''\n",
        "    # SOLUTION\n",
        "    num_actions = envs.single_action_space.n\n",
        "    if rng.random() < epsilon:\n",
        "        return rng.integers(0, num_actions, size = (envs.num_envs,))\n",
        "    else:\n",
        "        q_scores = q_network(obs)\n",
        "        return q_scores.argmax(-1).detach().cpu().numpy()\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hige_Ki9GTVU"
      },
      "source": [
        "## Probe Environments\n",
        "\n",
        "Extremely simple probe environments are a great way to debug your algorithm. The first one is given to you.\n",
        "\n",
        "Let's try and break down how this environment works. We see that the function `step` always returns the same thing. The observation and reward are always the same, and `done` is always true (i.e. the episode always terminates after one action). We expect the agent to rapidly learn that the value of the constant observation `[0.0]` is `+1`.\n",
        "\n",
        "### A note on action spaces\n",
        "\n",
        "The action space we're using here is `gym.spaces.Box`. This means we're dealing with real-valued quantities, i.e. continuous not discrete. The first two arguments of `Box` are `low` and `high`, and these define a box in $\\mathbb{R}^n$. For instance, if these arrays are `(0, 0)` and `(1, 1)` respectively, this defines the box $0 \\leq x, y \\leq 1$ in 2D space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiqgBb5xGTVU"
      },
      "outputs": [],
      "source": [
        "ObsType = np.ndarray\n",
        "ActType = int\n",
        "\n",
        "\n",
        "class Probe1(gym.Env):\n",
        "    '''One action, observation of [0.0], one timestep long, +1 reward.\n",
        "\n",
        "    We expect the agent to rapidly learn that the value of the constant [0.0] observation is +1.0. Note we're using a continuous observation space for consistency with CartPole.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.observation_space = Box(np.array([0]), np.array([0]))\n",
        "        self.action_space = Discrete(1)\n",
        "        self.seed()\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        return (np.array([0]), 1.0, True, {})\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        super().reset(seed=seed)\n",
        "        if return_info:\n",
        "            return (np.array([0.0]), {})\n",
        "        return np.array([0.0])\n",
        "\n",
        "\n",
        "gym.envs.registration.register(id=\"Probe1-v0\", entry_point=Probe1)\n",
        "env = gym.make(\"Probe1-v0\")\n",
        "assert env.observation_space.shape == (1,)\n",
        "assert env.action_space.shape == ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9bbXj9XGTVU"
      },
      "source": [
        "### Exercise - implement additional probe environments\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "Importance: ðŸŸ âšªâšªâšªâšª\n",
        "\n",
        "You should spend up to 10-30 minutes on this exercise.\n",
        "\n",
        "You should look at solutions if you're stuck. It's very important to have working probe environments to debug your algorithm.\n",
        "```\n",
        "\n",
        "Feel free to skip ahead for now, and implement these as needed to debug your model.\n",
        "\n",
        "Each implementation should be very similar to `Probe1` above. If you aren't sure whether you've implemented them correctly, you can check them against the solutions. Make sure you check them againts the solutions eventually - it's hard enough debugging your RL algorithms without also worrying about whether the probe environments you're using for debugging are correct!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7b800a3GTVU"
      },
      "outputs": [],
      "source": [
        "class Probe2(gym.Env):\n",
        "    '''One action, observation of [-1.0] or [+1.0], one timestep long, reward equals observation.\n",
        "\n",
        "    We expect the agent to rapidly learn the value of each observation is equal to the observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        pass\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        pass\n",
        "\n",
        "gym.envs.registration.register(id=\"Probe2-v0\", entry_point=Probe2)\n",
        "\n",
        "\n",
        "class Probe3(gym.Env):\n",
        "    '''One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.\n",
        "\n",
        "    We expect the agent to rapidly learn the discounted value of the initial observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        pass\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        pass\n",
        "\n",
        "gym.envs.registration.register(id=\"Probe3-v0\", entry_point=Probe3)\n",
        "\n",
        "\n",
        "class Probe4(gym.Env):\n",
        "    '''Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on the action.\n",
        "\n",
        "    We expect the agent to learn to choose the +1.0 action.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        pass\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        pass\n",
        "\n",
        "gym.envs.registration.register(id=\"Probe4-v0\", entry_point=Probe4)\n",
        "\n",
        "\n",
        "class Probe5(gym.Env):\n",
        "    '''Two actions, random 0/1 observation, one timestep, reward is 1 if action equals observation otherwise -1.\n",
        "\n",
        "    We expect the agent to learn to match its action to the observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        pass\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        pass\n",
        "\n",
        "gym.envs.registration.register(id=\"Probe5-v0\", entry_point=Probe5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ5Z2EIcGTVV"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class Probe2(gym.Env):\n",
        "    '''One action, observation of [-1.0] or [+1.0], one timestep long, reward equals observation.\n",
        "\n",
        "    We expect the agent to rapidly learn the value of each observation is equal to the observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        # SOLUTION\n",
        "        super().__init__()\n",
        "        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))\n",
        "        self.action_space = Discrete(1)\n",
        "        self.reset()\n",
        "        self.reward = None\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        # SOLUTION\n",
        "        assert self.reward is not None\n",
        "        return np.array([self.observation]), self.reward, True, {}\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        # SOLUTION\n",
        "        super().reset(seed=seed)\n",
        "        self.reward = 1.0 if self.np_random.random() < 0.5 else -1.0\n",
        "        self.observation = self.reward\n",
        "        if return_info:\n",
        "            return np.array([self.reward]), {}\n",
        "        return np.array([self.reward])\n",
        "\n",
        "class Probe3(gym.Env):\n",
        "    '''One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.\n",
        "\n",
        "    We expect the agent to rapidly learn the discounted value of the initial observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        # SOLUTION\n",
        "        super().__init__()\n",
        "        self.observation_space = Box(np.array([-0.0]), np.array([+1.0]))\n",
        "        self.action_space = Discrete(1)\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        # SOLUTION\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            return np.array([1.0]), 0.0, False, {}\n",
        "        elif self.n == 2:\n",
        "            return np.array([0.0]), 1.0, True, {}\n",
        "        raise ValueError(self.n)\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        # SOLUTION\n",
        "        super().reset(seed=seed)\n",
        "        self.n = 0\n",
        "        if return_info:\n",
        "            return np.array([0.0]), {}\n",
        "        return np.array([0.0])\n",
        "\n",
        "class Probe4(gym.Env):\n",
        "    '''Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on the action.\n",
        "\n",
        "    We expect the agent to learn to choose the +1.0 action.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        # SOLUTION\n",
        "        self.observation_space = Box(np.array([-0.0]), np.array([+0.0]))\n",
        "        self.action_space = Discrete(2)\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        # SOLUTION\n",
        "        reward = -1.0 if action == 0 else 1.0\n",
        "        return np.array([0.0]), reward, True, {}\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        # SOLUTION\n",
        "        super().reset(seed=seed)\n",
        "        if return_info:\n",
        "            return np.array([0.0]), {}\n",
        "        return np.array([0.0])\n",
        "\n",
        "class Probe5(gym.Env):\n",
        "    '''Two actions, random 0/1 observation, one timestep, reward is 1 if action equals observation otherwise -1.\n",
        "\n",
        "    We expect the agent to learn to match its action to the observation.\n",
        "    '''\n",
        "\n",
        "    action_space: Discrete\n",
        "    observation_space: Box\n",
        "\n",
        "    def __init__(self):\n",
        "        # SOLUTION\n",
        "        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))\n",
        "        self.action_space = Discrete(2)\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
        "        # SOLUTION\n",
        "        reward = 1.0 if action == self.obs else -1.0\n",
        "        return np.array([self.obs]), reward, True, {}\n",
        "\n",
        "    def reset(\n",
        "        self, seed: Optional[int] = None, return_info=False, options=None\n",
        "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
        "        # SOLUTION\n",
        "        super().reset(seed=seed)\n",
        "        self.obs = 1.0 if self.np_random.random() < 0.5 else 0.0\n",
        "        if return_info:\n",
        "            return np.array([self.obs], dtype=float), {}\n",
        "        return np.array([self.obs], dtype=float)\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXJOcp_WGTVV"
      },
      "source": [
        "## Weights and Biases\n",
        "\n",
        "In previous parts, we've just trained the agent, and then plotted the reward per episode after training. For small toy examples that train in a few seconds this is fine, but for longer runs we'd like to watch the run live and make sure the agent is doing something interesting (especially if we were planning to run the model overnight.)\n",
        "\n",
        "Luckily, **Weights and Biases** has got us covered! When you run your experiments, you'll be able to view not only *live plots* of the loss and average reward per episode while the agent is training - you can also log and view animations, which visualise your agent's progress in real time! The code below will handle all logging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn4STeLKGTVV"
      },
      "source": [
        "## Main DQN Algorithm\n",
        "\n",
        "We now combine all the elements we have designed thus far into the final DQN algorithm. Here, we assume the environment returns three parameters $(s_{new}, r, d)$, a new state $s_{new}$, a reward $r$ and a boolean $d$ indicating whether interaction has terminated yet.\n",
        "\n",
        "Our Q-value function $Q(s,a)$ is now a network $Q(s,a ; \\theta)$ parameterised by weights $\\theta$. The key idea, as in Q-learning, is to ensure the Q-value function satisfies the optimal Bellman equation\n",
        "$$\n",
        "Q(s,a ; \\theta)\n",
        "= \\mathbb{E}_{s',r \\sim p(\\cdot \\mid s,a)} \\left[r + \\gamma \\max_{a'} Q(s', a' ;\\theta) \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\delta_t = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
        "$$\n",
        "Letting $y_t = r_t + \\gamma \\max_a Q(s_{t+1}, a)$, we can use the expected squared TD-Error $\\delta_t^2 = (y_t - Q(s_t, a_t))^2$ as the loss function to optimize against. Since we want the modle to learn from a variety of experiences (recall that supervised learning is assuming i.i.d) we approximate the expectation by sampling a batch $B = \\{s^i, a^i, r^i, s^i_\\text{new}\\}$ of experiences from the replay buffer, and try to adjust $\\theta$ to make the loss\n",
        "$$\n",
        "L(\\theta) = \\frac{1}{|B|} \\sum_{i=1}^B \\left( r^i +\n",
        "\\gamma \\max_a Q(s^i_\\text{new}, a ; \\theta_\\text{target}) - Q(s^i, a^i ; \\theta) \\right)^2\n",
        "$$\n",
        "smaller via gradient descent. Here, $\\theta_\\text{target}$ is a previous copy of the parameters $\\theta$. Every so often, we then update the target $\\theta_\\text{target} \\leftarrow \\theta$ as the agent improves it's Q-values from experience.\n",
        "\n",
        "The image below uses the notation $[[ S ]]$ - this means 1 if $S$ is True, and 0 if $S$ is False.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn_algo.png\" width=\"550\">\n",
        "\n",
        "On line 13, we need to check if the environment is still running before adding the $\\gamma \\max_{a'}Q(s^i_\\text{new}, a' ; \\theta_\\text{target})$ term, as terminal states don't have future rewards, so we zero the second term if $d^i = \\text{True}$, indicating that the episode has terminated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZiYnEmKGTVV"
      },
      "source": [
        "Below is a dataclass for training your DQN. You can use the `arg_help` method to see a description of each argument (it will also highlight any arguments which have ben changed from their default values).\n",
        "\n",
        "The exact breakdown of training is as follows:\n",
        "\n",
        "* The agent takes `total_timesteps` steps in the environment during the training loop.\n",
        "* The first `buffer_size` of these steps are used to fill the replay buffer (we don't update gradients until the buffer is full).\n",
        "* After this point, we perform an optimizer step every `train_frequency` steps of our agent.\n",
        "\n",
        "This is shown in the diagram below (not to scale!).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn_breakdown.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAYE95NbGTVW"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DQNArgs:\n",
        "    exp_name: str = \"DQN_implementation\"\n",
        "    seed: int = 1\n",
        "    torch_deterministic: bool = True\n",
        "    cuda: bool = t.cuda.is_available()\n",
        "    log_dir: str = \"logs\"\n",
        "    use_wandb: bool = False\n",
        "    wandb_project_name: str = \"CartPoleDQN\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    capture_video: bool = True\n",
        "    env_id: str = \"CartPole-v1\"\n",
        "    total_timesteps: int = 500_000\n",
        "    learning_rate: float = 0.00025\n",
        "    buffer_size: int = 10_000\n",
        "    gamma: float = 0.99\n",
        "    target_network_frequency: int = 500\n",
        "    batch_size: int = 128\n",
        "    start_e: float = 1.0\n",
        "    end_e: float = 0.1\n",
        "    exploration_fraction: float = 0.2\n",
        "    train_frequency: int = 10\n",
        "    log_frequency: int = 50\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.total_timesteps - self.buffer_size >= self.train_frequency\n",
        "        self.total_training_steps = (self.total_timesteps - self.buffer_size) // self.train_frequency\n",
        "\n",
        "\n",
        "args = DQNArgs(batch_size=256)\n",
        "utils.arg_help(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzAun6kgGTVW"
      },
      "source": [
        "### Exercise - fill in the agent class\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 25-49 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should now fill in the methods for the `DQNAgent` class below. This is a class which is designed to handle taking steps in the environment (with an epsilon greedy policy), and updating the buffer.\n",
        "\n",
        "The `play_step` function should do the following:\n",
        "\n",
        "* Get a new set of actions via the `self.get_actions` method (taking `self.next_obs` as our current observation)\n",
        "* Step the environment, via `self.envs.step` (which returns a new set of experiences)\n",
        "* Add the new experiences to the buffer\n",
        "* Set `self.next_obs` to the new observations (this is so the agent knows where it is for the next step)\n",
        "* Increment the global step counter\n",
        "* Return the diagnostic information from the new experiences (i.e. the `infos` dicts)\n",
        "\n",
        "The `get_actions` function should do the following:\n",
        "\n",
        "* Set `self.epsilon` according to the linear schedule, and current timestep\n",
        "* Sample actions according to the epsilon-greedy policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgLbQkVBGTVW"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"Base Agent class handling the interaction with the environment.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        envs: gym.vector.SyncVectorEnv,\n",
        "        args: DQNArgs,\n",
        "        rb: ReplayBuffer,\n",
        "        q_network: QNetwork,\n",
        "        target_network: QNetwork,\n",
        "        rng: np.random.Generator\n",
        "    ):\n",
        "        self.envs = envs\n",
        "        self.args = args\n",
        "        self.rb = rb\n",
        "        self.next_obs = self.envs.reset() # Need a starting observation!\n",
        "        self.steps = 0\n",
        "        self.epsilon = args.start_e\n",
        "        self.q_network = q_network\n",
        "        self.target_network = target_network\n",
        "        self.rng = rng\n",
        "\n",
        "    def play_step(self) -> List[dict]:\n",
        "        '''\n",
        "        Carries out a single interaction step between the agent and the environment, and adds results to the replay buffer.\n",
        "\n",
        "        Returns `infos` (list of dictionaries containing info we will log).\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def get_actions(self, obs: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "tests.test_agent(DQNAgent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bXdKvCmGTVW"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class DQNAgent:\n",
        "    \"\"\"Base Agent class handling the interaction with the environment.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        envs: gym.vector.SyncVectorEnv,\n",
        "        args: DQNArgs,\n",
        "        rb: ReplayBuffer,\n",
        "        q_network: QNetwork,\n",
        "        target_network: QNetwork,\n",
        "        rng: np.random.Generator\n",
        "    ):\n",
        "        self.envs = envs\n",
        "        self.args = args\n",
        "        self.rb = rb\n",
        "        self.next_obs = self.envs.reset() # Need a starting observation!\n",
        "        self.steps = 0\n",
        "        self.epsilon = args.start_e\n",
        "        self.q_network = q_network\n",
        "        self.target_network = target_network\n",
        "        self.rng = rng\n",
        "\n",
        "    def play_step(self) -> List[dict]:\n",
        "        '''\n",
        "        Carries out a single interaction step between the agent and the environment, and adds results to the replay buffer.\n",
        "\n",
        "        Returns `infos` (list of dictionaries containing info we will log).\n",
        "        '''\n",
        "        # SOLUTION\n",
        "        obs = self.next_obs\n",
        "        actions = self.get_actions(obs)\n",
        "        next_obs, rewards, dones, infos = self.envs.step(actions)\n",
        "        self.rb.add(obs, actions, rewards, dones, next_obs)\n",
        "\n",
        "        self.next_obs = next_obs\n",
        "        self.steps += 1\n",
        "        return infos\n",
        "\n",
        "    def get_actions(self, obs: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.\n",
        "        '''\n",
        "        # SOLUTION\n",
        "        self.epsilon = linear_schedule(self.steps, args.start_e, args.end_e, args.exploration_fraction, args.total_timesteps)\n",
        "        actions = epsilon_greedy_policy(self.envs, self.q_network, self.rng, t.tensor(obs).to(device), self.epsilon)\n",
        "        assert actions.shape == (len(self.envs.envs),)\n",
        "        return actions\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_NDILdNGTVW"
      },
      "source": [
        "Now we'll create a new class `DQNTrainer`, which will handle the full training loop. We've filled in the `__init__` for you, which defines the following important attributes (among others):\n",
        "\n",
        "* `q_network` and `target_network`, which you'll use to step and train the agent.\n",
        "* `optimizer`, which is used during the training steps.\n",
        "* `rb`, the replay buffer.\n",
        "* `agent`, to handle stepping the agent in the environment.\n",
        "\n",
        "You should fill in the remaining 2 methods:\n",
        "\n",
        "#### `add_to_replay_buffer`\n",
        "\n",
        "This takes an argument `n`, and makes the agent take `n` steps in the environment (also logging any important variables).\n",
        "\n",
        "Note that we call it once at the start of training, to fill the buffer. We'll also call it before each training step, to add `args.train_frequency` new experiences to the buffer.\n",
        "\n",
        "#### `training_step`\n",
        "\n",
        "This samples data from the buffer using `self.rb.sample`, and then performs an update step on the agent. This involves:\n",
        "* Getting the max of the target network for the next observations (in inference mode),\n",
        "* Getting the predicted Q-values,\n",
        "* Using these to calculate the TD loss,\n",
        "* Perform a gradient step on the loss,\n",
        "* If `self.agent.steps` divides `args.target_network_frequency`, then load the weights from the Q-network into the target network,\n",
        "* Log any important variables.\n",
        "\n",
        "A few tips for both of these functions:\n",
        "\n",
        "* You can log variables using `wandb.log({\"variable_name\": variable_value}, steps=steps)`.\n",
        "* The `agent.play_step()` method returns a list of dictionaries containing data about the current run. If the agent terminated at that step, then the dictionary will contain `{\"episode\": {\"l\": episode_length, \"r\": episode_reward}}`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8oU4dFGTVW"
      },
      "source": [
        "### Exercise - write DQN training loop\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ ðŸŸ \n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ ðŸŸ \n",
        "\n",
        "You should spend up to 30-60 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Don't be discouraged if your code takes a while to work - it's normal for debugging RL to take longer than you would expect. Add asserts or your own tests, implement an appropriate probe environment, try anything in the Andy Jones post that sounds promising, and try to notice confusion. Reinforcement Learning is often so tricky as even if the algorithm has bugs, the agent might still learn something useful regardless (albeit maybe not as well), or even if everything is correct, the agent might just fail to learn anything useful (like how DQN failed to do anything on Montezuma's Revenge.)\n",
        "\n",
        "Since the environment is already know to be one DQN can solve, and we've already provided hyperparameters that work for this environment, hopefully that's isolated a lot of the problems one would usually have with solving real world problems with RL.\n",
        "\n",
        "<details>\n",
        "<summary>Expected Behavior of the Loss</summary>\n",
        "\n",
        "In supervised learning, we want our loss to always be decreasing and it's a bad sign if it's actually increasing, as that's the only metric we care about. In RL, it's the total reward per epsiode that should be (noisily) increasing over time.\n",
        "\n",
        "Our agent's loss function just reflects how close together the Q-network's estimates are to the experiences currently sampled from the replay buffer, which might not adequately represent what the world actually looks like.\n",
        "\n",
        "This means that once the agent starts to learn something and do better at the problem, it's expected for the loss to increase. The loss here is just the TD-error, the difference between how valuable the agent thinks the (state-action) is, v.s. the best  current bootstrapped estimate of the actual Q-value.\n",
        "\n",
        "For example, the Q-network initially learned some state was bad, because an agent that reached them was just flapping around randomly and died shortly after. But now it's getting evidence that the same state is good, now that the agent that reached the state has a better idea what to do next. A higher loss is thus actually a good sign that something is happening (the agent hasn't stagnated), but it's not clear if it's learning anything useful without also checking how the total reward per episode has changed.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-LcITSAGTVX"
      },
      "outputs": [],
      "source": [
        "class DQNTrainer:\n",
        "\n",
        "    def __init__(self, args: DQNArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "        if args.use_wandb:\n",
        "            wandb.init(project=args.wandb_project_name, entity=args.wandb_entity, name=self.run_name)\n",
        "            if args.capture_video: wandb.gym.monitor()\n",
        "\n",
        "        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, self.run_name)])\n",
        "        self.start_time = time.time()\n",
        "        self.rng = np.random.default_rng(args.seed)\n",
        "\n",
        "        num_actions = self.envs.single_action_space.n\n",
        "        obs_shape = self.envs.single_observation_space.shape\n",
        "        num_observations = np.array(obs_shape, dtype=int).prod()\n",
        "\n",
        "        self.q_network = QNetwork(num_observations, num_actions).to(device)\n",
        "        self.target_network = QNetwork(num_observations, num_actions).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.optimizer = t.optim.Adam(self.q_network.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        self.rb = ReplayBuffer(args.buffer_size, len(self.envs.envs), args.seed)\n",
        "        self.agent = DQNAgent(self.envs, self.args, self.rb, self.q_network, self.target_network, self.rng)\n",
        "\n",
        "        self.add_to_replay_buffer(args.buffer_size)\n",
        "\n",
        "\n",
        "    def add_to_replay_buffer(self, n: int):\n",
        "        '''Makes n steps, adding to the replay buffer (and logging any results).'''\n",
        "        pass\n",
        "\n",
        "    def training_step(self) -> Float[Tensor, \"\"]:\n",
        "        '''Samples once from the replay buffer, and takes a single training step.'''\n",
        "        pass\n",
        "\n",
        "def train(args: DQNArgs) -> QNetwork:\n",
        "    trainer = DQNTrainer(args)\n",
        "    progress_bar = tqdm(range(args.total_training_steps))\n",
        "    for step in progress_bar:\n",
        "        last_episode_len = trainer.add_to_replay_buffer(args.train_frequency)\n",
        "        if last_episode_len is not None:\n",
        "            progress_bar.set_description(f\"Step = {trainer.agent.steps}, Episodic return = {last_episode_len}\")\n",
        "        trainer.training_step()\n",
        "    return trainer.q_network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYWMMACFGTVX"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "class DQNTrainer:\n",
        "\n",
        "    def __init__(self, args: DQNArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "        if args.use_wandb:\n",
        "            wandb.init(project=args.wandb_project_name, entity=args.wandb_entity, name=args.exp_name)\n",
        "\n",
        "        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, self.run_name)])\n",
        "        self.start_time = time.time()\n",
        "        self.rng = np.random.default_rng(args.seed)\n",
        "\n",
        "        num_actions = self.envs.single_action_space.n\n",
        "        obs_shape = self.envs.single_observation_space.shape\n",
        "        num_observations = np.array(obs_shape, dtype=int).prod()\n",
        "\n",
        "        self.q_network = QNetwork(num_observations, num_actions).to(device)\n",
        "        self.target_network = QNetwork(num_observations, num_actions).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.optimizer = t.optim.Adam(self.q_network.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        self.rb = ReplayBuffer(args.buffer_size, len(self.envs.envs), args.seed)\n",
        "        self.agent = DQNAgent(self.envs, self.args, self.rb, self.q_network, self.target_network, self.rng)\n",
        "\n",
        "        self.add_to_replay_buffer(args.buffer_size)\n",
        "\n",
        "\n",
        "    def add_to_replay_buffer(self, n: int):\n",
        "        '''Makes n steps, adding to the replay buffer (and logging any results).'''\n",
        "        # SOLUTION\n",
        "        last_episode_len = None\n",
        "        for step in range(n):\n",
        "            infos = self.agent.play_step()\n",
        "            for info in infos:\n",
        "                if \"episode\" in info.keys():\n",
        "                    last_episode_len = info[\"episode\"][\"l\"]\n",
        "                    if self.args.use_wandb:\n",
        "                        wandb.log({\"episode_len\": last_episode_len}, step=self.agent.steps)\n",
        "        return last_episode_len\n",
        "\n",
        "\n",
        "    def training_step(self) -> Float[Tensor, \"\"]:\n",
        "        '''Samples once from the replay buffer, and takes a single training step.'''\n",
        "        # SOLUTION\n",
        "        data = self.rb.sample(self.args.batch_size, device)\n",
        "        s, a, r, d, s_new = data.observations, data.actions, data.rewards, data.dones, data.next_observations\n",
        "\n",
        "        with t.inference_mode():\n",
        "            target_max = self.target_network(s_new).max(-1).values\n",
        "        predicted_q_vals = self.q_network(s)[range(self.args.batch_size), a.flatten()]\n",
        "\n",
        "        td_error = r.flatten() + self.args.gamma * target_max * (1 - d.float().flatten()) - predicted_q_vals\n",
        "        loss = td_error.pow(2).mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        if self.agent.steps % self.args.target_network_frequency == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log(\n",
        "                {\"td_loss\": loss, \"q_values\": predicted_q_vals.mean().item(), \"SPS\": int(self.agent.steps / (time.time() - self.start_time))},\n",
        "                step=self.agent.steps\n",
        "            )\n",
        "\n",
        "\n",
        "def train(args: DQNArgs) -> QNetwork:\n",
        "    trainer = DQNTrainer(args)\n",
        "    progress_bar = tqdm(range(args.total_training_steps))\n",
        "    for step in progress_bar:\n",
        "        last_episode_len = trainer.add_to_replay_buffer(args.train_frequency)\n",
        "        if last_episode_len is not None:\n",
        "            progress_bar.set_description(f\"Step = {trainer.agent.steps}, Episodic return = {last_episode_len}\")\n",
        "        trainer.training_step()\n",
        "    return trainer.q_network\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7W_OEuGTVX"
      },
      "source": [
        "Here's some boilerplate code to run one of your probes (change the `probe_idx` variable to try out different probes):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oguhbgHNGTVX"
      },
      "outputs": [],
      "source": [
        "def test_probe(probe_idx: int):\n",
        "    args = DQNArgs(\n",
        "        env_id=f\"Probe{probe_idx}-v0\",\n",
        "        exp_name=f\"test-probe-{probe_idx}\",\n",
        "        total_timesteps=2000 if probe_idx <= 2 else 5000,\n",
        "        learning_rate=0.001,\n",
        "        buffer_size=500,\n",
        "        capture_video=False,\n",
        "        use_wandb=False\n",
        "    )\n",
        "\n",
        "    obs_for_probes = [[[0.0]], [[-1.0], [+1.0]], [[0.0], [1.0]], [[0.0]], [[0.0], [1.0]]]\n",
        "    expected_value_for_probes = [[[1.0]], [[-1.0], [+1.0]], [[args.gamma], [1.0]], [[-1.0, 1.0]], [[1.0, -1.0], [-1.0, 1.0]]]\n",
        "    tolerances = [5e-4, 5e-4, 5e-4, 5e-4, 1e-3]\n",
        "    obs = t.tensor(obs_for_probes[probe_idx-1]).to(device)\n",
        "\n",
        "    # YOUR CODE HERE - create a PPOTrainer class, and train your agent\n",
        "    q_network = train(args)\n",
        "\n",
        "    value = q_network(obs)\n",
        "    expected_value = t.tensor(expected_value_for_probes[probe_idx-1]).to(device)\n",
        "    t.testing.assert_close(value, expected_value, atol=tolerances[probe_idx-1], rtol=0)\n",
        "    clear_output()\n",
        "    print(\"Probe tests passed!\")\n",
        "\n",
        "\n",
        "for probe_idx in range(6):\n",
        "    test_probe(probe_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7RJdd78GTVX"
      },
      "source": [
        "Once you've passed the tests for all 5 probe environments, you should test your model on Cartpole.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eusr4Go5GTVX"
      },
      "outputs": [],
      "source": [
        "args = DQNArgs(use_wandb=True)\n",
        "q_network = train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEbycf6lGTVX"
      },
      "source": [
        "## Beyond CartPole\n",
        "\n",
        "If things go well and your agent masters CartPole, the next harder challenges are [Acrobot-v1](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py), and [MountainCar-v0](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py). These also have discrete action spaces, which are the only type we're dealing with today. Feel free to Google for appropriate hyperparameters for these other problems - in a real RL problem you would have to do hyperparameter search using the techniques we learned on a previous day because bad hyperparameters in RL often completely fail to learn, even if the algorithm is perfectly correct.\n",
        "\n",
        "There are many more exciting environments to play in, but generally they're going to require more compute and more optimization than we have time for today. If you finish the main material, some we recommend are:\n",
        "\n",
        "- [Minimalistic Gridworld Environments](https://github.com/Farama-Foundation/gym-minigrid) - a fast gridworld environment for experiments with sparse rewards and natural language instruction.\n",
        "- [microRTS](https://github.com/santiontanon/microrts) - a small real-time strategy game suitable for experimentation.\n",
        "- [Megastep](https://andyljones.com/megastep/) - RL environment that runs fully on the GPU (fast!)\n",
        "- [Procgen](https://github.com/openai/procgen) - A family of 16 procedurally generated gym environments to measure the ability for an agent to generalize. Optimized to run quickly on the CPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGq7EvwIGTVX"
      },
      "source": [
        "\n",
        "## Bonus\n",
        "\n",
        "### Target Network\n",
        "\n",
        "Why have the target network? Modify the DQN code above, but this time use the same network for both the target and the Q-value network, rather than updating the target every so often.\n",
        "\n",
        "Compare the performance of this against using the target network.\n",
        "\n",
        "### Shrink the Brain\n",
        "\n",
        "Can DQN still learn to solve CartPole with a Q-network with fewer parameters? Could we get away with three-quarters or even half as many parameters? Try comparing the resulting training curves with a shrunken version of the Q-network. What about the same number of parameters, but with more/less layers, and less/more parameters per layer?\n",
        "\n",
        "### Dueling DQN\n",
        "\n",
        "Implement dueling DQN according to [the paper](https://arxiv.org/pdf/1511.06581.pdf) and compare its performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jd3LpCav3UXu",
        "XcgAnZZOyBYk",
        "RMHgKpawGTVI",
        "wIExEmUTGTVN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}